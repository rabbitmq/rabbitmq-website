<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!--
Copyright (c) 2007-2019 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "License”); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
      xmlns:x="http://www.rabbitmq.com/2011/extensions">
  <head>
    <title>Clustering and Network Partitions</title>
  </head>
  <body show-in-this-page="true">
    <doc:section name="intro">
      <doc:heading>Introduction</doc:heading>

      <p>
        This guide covers one specific aspect of clustering: network
        failures between nodes, their effects and recovery options.
        For a general overview of clustering, see <a href="/clustering.html">Clustering</a>
        and <a href="/cluster-formation.html">Peer Discovery and Cluster Formation</a> guides.
      </p>

      <p>
        Clustering can be used to achieve different goals: increased
        data safety through replication, increased availability for
        client operations, higher overall throughput and so on.
        Different configurations are optimal for different purposes.
      </p>

      <p>
        Network connection failures between cluster members have an effect on
        data consistency and availability (as in the CAP theorem) to client operations.
        Since different applications have different requirements around consistency
        and can tolerate unavailability to a different extent, different
        <a href="#automatic-handling">partition handling strategies</a> are available.
      </p>
    </doc:section>

    <doc:section name="detecting">
      <doc:heading>Detecting Network Partitions</doc:heading>
      <p>
        Nodes determine if its peer is down if another
        node is unable to contact it for a <a href="nettick.html">period of time</a>, 60 seconds by default.
        If two nodes come back into contact, both having thought the other is down, the nodes will
        determine that a partition has occurred. This will be written to
        the RabbitMQ log in a form like:
      </p>
<pre class="sourcecode">
=ERROR REPORT==== 15-Oct-2012::18:02:30 ===
Mnesia(rabbit@hostname): ** ERROR ** mnesia_event got
{inconsistent_database, running_partitioned_network, hare@hostname}
</pre>

      <p>
        Partition presence can be idenfied via server <a href="/logging.html">logs</a>,
        <a href="/management.html">HTTP API</a> (for <a href="/monitoring.html">monitoring</a>)
        and a <a href="/cli.html">CLI command</a>, <code>rabbitmqctl cluster_status</code>.
      </p>
      <p>
        <code>rabbitmqctl cluster_status</code> will normally show an
        empty list for <code>partitions</code>:
      </p>
<pre class="lang-bash">
rabbitmqctl cluster_status
# => Cluster status of node rabbit@smacmullen ...
# => [{nodes,[{disc,[hare@smacmullen,rabbit@smacmullen]}]},
# =>  {running_nodes,[rabbit@smacmullen,hare@smacmullen]},
# =>  {partitions,[]}]
# => ...done.
</pre>
      <p>
        However, if a network partition has occurred then information
        about partitions will appear there:
      </p>
<pre class="lang-bash">
rabbitmqctl cluster_status
# => Cluster status of node rabbit@smacmullen ...
# => [{nodes,[{disc,[hare@smacmullen,rabbit@smacmullen]}]},
# =>  {running_nodes,[rabbit@smacmullen,hare@smacmullen]},
# =>  {partitions,[{rabbit@smacmullen,[hare@smacmullen]},
# =>               {hare@smacmullen,[rabbit@smacmullen]}]}]
# => ...done.
</pre>
      <p>
        The HTTP API will return partition
        information for each node under <code>partitions</code>
        in <code>/api/nodes</code>. The management UI will show a
        warning on the overview page if a partition has occurred.
      </p>
    </doc:section>

    <doc:section name="during">
      <doc:heading>During a Network Partition</doc:heading>
      <p>
        While a network partition is in place, the two (or more!) sides
        of the cluster can evolve independently, with both sides
        thinking the other has crashed. This scenario is known as split-brain.
        Queues, bindings, exchanges can
        be created or deleted separately. <a href="ha.html">Mirrored
        queues</a> which are split across the partition will end up with
        one master on each side of the partition, again with both sides
        acting independently. Unless a <a href="#automatic-handling">partition handling strategy</a>,
        such as <code>pause_minority</code>, is configured to be used,
        the split will continue even after network connectivity is restored.
      </p>
    </doc:section>

    <doc:section name="suspend">
      <doc:heading>Partitions Caused by Suspend and Resume</doc:heading>
      <p>
        While we refer to "network" partitions, really a partition is
        any case in which the different nodes of a cluster can have
        communication interrupted without any node failing. In addition
        to network failures, suspending and resuming an entire OS can
        also cause partitions when used against running cluster nodes -
        as the suspended node will not consider itself to have failed, or
        even stopped, but the other nodes in the cluster will consider
        it to have done so.
      </p>

      <p>
        While you could suspend a cluster node by running it on a laptop
        and closing the lid, the most common reason for this to happen
        is for a virtual machine to have been suspended by the
        hypervisor.
        <b>While it's fine to run RabbitMQ clusters in
        virtualised environments, you should make sure that VMs are not
        suspended while running. Note that some virtualisation features
        such as migration of a VM from one host to another will tend to
        involve the VM being suspended.</b>
      </p>

      <p>
        Partitions caused by suspend and resume will tend to be
        asymmetrical - the suspended node will not necessarily see the
        other nodes as having gone down, but will be seen as down by the
        rest of the cluster. This has particular implications for <a
        href="#pause-minority">pause_minority</a> mode.
      </p>
    </doc:section>

    <doc:section name="recovering">
      <doc:heading>Recovering From a Split-Brain</doc:heading>
      <p>
        To recover from a split-brain, first choose one partition
        which you trust the most. This partition will become the
        authority for the state of the system (schema, messages)
        to use; any changes which have occurred on other partitions will be lost.
      </p>
      <p>
        Stop all nodes in the other partitions, then start them all up
        again. When they rejoin the cluster they will restore state from
        the trusted partition.
      </p>
      <p>
        Finally, you should also restart all the nodes in the trusted
        partition to clear the warning.
      </p>
      <p>
        It may be simpler to stop the whole cluster and start it again;
        if so make sure that the <b>first</b> node you start is from the
        trusted partition.
      </p>
    </doc:section>

    <doc:section name="automatic-handling">
      <doc:heading>Partition Handling Strategies</doc:heading>
      <p>
        RabbitMQ also three ways to deal with network partitions
        automatically: <code>pause-minority</code> mode, <code>pause-if-all-down</code>
        mode and <code>autoheal</code> mode. The default behaviour is referred
        to as <code>ignore</code> mode.
      </p>

      <p>
        In pause-minority mode RabbitMQ will automatically pause cluster
        nodes which determine themselves to be in a minority (i.e. fewer
        or equal than half the total number of nodes) after seeing other
        nodes go down. It therefore chooses partition tolerance over
        availability from the CAP theorem. This ensures that in the
        event of a network partition, at most the nodes in a single
        partition will continue to run. The minority nodes will pause as
        soon as a partition starts, and will start again when the
        partition ends. This configuration prevents split-brain and
        is therefore able to automatically recover from
        network partitions without inconsistencies.
      </p>

      <p>
        In pause-if-all-down mode, RabbitMQ will automatically pause
        cluster nodes which cannot reach any of the listed nodes. In
        other words, all the listed nodes must be down for RabbitMQ to
        pause a cluster node. This is close to the pause-minority mode,
        however, it allows an administrator to decide which nodes to
        prefer, instead of relying on the context. For instance, if the
        cluster is made of two nodes in rack A and two nodes in rack B,
        and the link between racks is lost, pause-minority mode will pause
        all nodes. In pause-if-all-down mode, if the administrator listed
        the two nodes in rack A, only nodes in rack B will pause. Note
        that it is possible the listed nodes get split across both sides
        of a partition: in this situation, no node will pause. That is why
        there is an additional <i>ignore</i>/<i>autoheal</i> argument to
        indicate how to recover from the partition.
      </p>

      <p>
        In autoheal mode RabbitMQ will automatically decide on a winning
        partition if a partition is deemed to have occurred, and will
        restart all nodes that are not in the winning partition. Unlike
        pause_minority mode it therefore takes effect when a partition
        ends, rather than when one starts.
      </p>
      <p>
        The winning partition is the one which has the most clients
        connected (or if this produces a draw, the one with the most
        nodes; and if that still produces a draw then one of the
        partitions is chosen in an unspecified way).
      </p>

      <p>
        You can enable either mode by setting the configuration
        parameter <code>cluster_partition_handling</code> for
        the <code>rabbit</code> application in
        your <a href="configure.html#configuration-file">configuration
        file</a> to:
      </p>
      <ul>
        <li><code>autoheal</code></li>
        <li><code>pause_minority</code></li>
        <li><code>pause_if_all_down</code></li>
      </ul>

      If using the <code>pause_if_all_down</code> mode, additional
      parameters are required:

      <ul>
        <li><code>nodes</code> - nodes, which should be unavailable to pause</li>
        <li><code>recover</code> - recover action, can be <code>ignore</code> or <code>autoheal</code></li>
      </ul>

      Example <a href="/configure.html#config-file">config snippet</a> that uses <code>pause_if_all_down</code>:

<pre class="sourcecode">
cluster_partition_handling = pause_if_all_down

## Recovery strategy. Can be either 'autoheal' or 'ignore'
cluster_partition_handling.pause_if_all_down.recover = ignore

## Node names to check
cluster_partition_handling.pause_if_all_down.nodes.1 = rabbit@myhost1
cluster_partition_handling.pause_if_all_down.nodes.2 = rabbit@myhost2
</pre>

    <h3 id="which-mode">Which mode should I pick?</h3>

    <p>
      It's important to understand that allowing RabbitMQ to deal with
      network partitions automatically does not make them less of a
      problem. Network partitions will always cause problems for
      RabbitMQ clusters; you just get some degree of choice over what
      kind of problems you get. As stated in the introduction, if you
      want to connect RabbitMQ clusters over generally unreliable
      links, you should use federation or the shovel.
    </p>

    <p>
      With that said, you might wish to pick a recovery mode as
      follows:
    </p>

    <ul>
      <li>
        <code>ignore</code> - Your network really is reliable. All
        your nodes are in a rack, connected with a switch, and that
        switch is also the route to the outside world. You don't want
        to run any risk of any of your cluster shutting down if any
        other part of it fails (or you have a two node cluster).
      </li>
      <li>
        <code>pause_minority</code> - Your network is maybe less
        reliable. You have clustered across three datacenters, and you
        assume that only one datacenter will fail at once. In that scenario
        you want the remaining two datacenters to continue working and the
        nodes from the failed datacenter to rejoin automatically and without
        fuss when the datacenter comes back. Those datacenters have to have
        a direct and reliable connection to each other, like availability zones
        in EC2.
      </li>
      <li>
        <code>autoheal</code> - Your network may not be reliable. You
        are more concerned with continuity of service than with data
        integrity. You may have a two node cluster.
      </li>
    </ul>

    <h3 id="pause-minority">More about pause-minority mode</h3>
    <p>
      The Erlang VM on the paused nodes will continue running but the
      nodes will not listen on any ports or do any other work. They
      will check once per second to see if the rest of the cluster has
      reappeared, and start up again if it has.
    </p>
    <p>
      Note that nodes will not enter the paused state at startup, even
      if they are in a minority then. It is expected that any such
      minority at startup is due to the rest of the cluster not having
      been started yet.
    </p>
    <p>
      Also note that RabbitMQ will pause nodes which are not in a
      <i>strict</i> majority of the cluster - i.e. containing more
      than half of all nodes. It is therefore not a good idea to
      enable pause-minority mode on a cluster of two nodes since in
      the event of any network partition <b>or node failure</b>, both
      nodes will pause. However, <code>pause_minority</code> mode is
      safer than <code>ignore</code> mode, with regards to integrity.
      For clusters of more than two nodes, especially if the most likely
      form of network partition is that a single minority of nodes
      drops off the network, the availability is remains as good as
      with <code>ignore</code> mode.
    </p>
    <p>
      Finally, note that <code>pause_minority</code> mode will do
      nothing to defend against partitions caused by cluster nodes
      being <a href="#suspend">suspended</a>. This is because
      the suspended node will never see the rest of the cluster
      vanish, so will have no trigger to disconnect itself from the
      cluster.
    </p>
    </doc:section>
  </body>
</html>
