<?xml-stylesheet type="text/xml" href="page.xsl"?>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
      xmlns:x="http://www.rabbitmq.com/2011/extensions">
  <head>
    <title>Clustering Guide</title>
  </head>
  <body>
      <doc:section name="clustering">
        <p>
          A RabbitMQ <i>broker</i> is a logical grouping of one or
          several Erlang <i>nodes</i>, each running the RabbitMQ
          <i>application</i> and sharing users, virtual hosts,
          queues, exchanges, etc. Sometimes we refer to the collection
          of nodes as a <i>cluster</i>.
        </p>
        <p>
          All data/state required for the operation of a RabbitMQ
          broker is replicated across all nodes, for reliability and
          scaling, with full ACID properties. An exception to this are
          message queues, which by default reside on the node that
          created them, though they are visible and reachable from all
          nodes. To replicate queues across nodes in a cluster, see
          the documentation on <a href="ha.html">high availability</a>
          (note that you will need a working cluster first).
        </p>
        <p>
          <b>RabbitMQ clustering does not tolerate network partions
          well</b>, so it should not be used over a WAN.
          The <a href="plugins.html#rabbitmq_shovel">shovel</a> or
          <a href="plugins.html#rabbitmq_federation">federation</a>
          plugins are better solutions for connecting brokers across a
          WAN.
        </p>
        <p>
          The easiest way to set up a cluster is by auto configuration
          using a default cluster config file. See the <a
          href="#auto-config">clustering
          transcripts</a> for an example.
        </p>
        <p>
          The composition of a cluster can be altered dynamically.
          All RabbitMQ brokers start out as running on a single
          node. These nodes can be joined into clusters, and
          subsequently turned back into individual brokers again.
        </p>
        <p>
          RabbitMQ brokers tolerate the failure of individual
          nodes. Nodes can be started and stopped at will.
        </p>
        <p>
          A node can be a <em>RAM node</em> or a <em>disk
          node</em>. RAM nodes keep their state only in memory (with
          the exception of the persistent contents of durable queues
          which are still stored safely on disc). Disk nodes keep
          state in memory and on disk. As RAM nodes don't have to
          write to disk as much as disk nodes, they can perform
          better. Because state is replicated across all nodes in the
          cluster, it is sufficient to have just one disk node within
          a cluster, to store the state of the cluster safely. Beware,
          however, that RabbitMQ will not stop you from creating a
          cluster with only RAM nodes. Should you do this, and suffer
          a power failure to the entire cluster, the entire state of
          the cluster, including all messages, will be lost.
        </p>
      </doc:section>

      <doc:section name="transcript">
        <doc:heading>Clustering transcript</doc:heading>
        <p>
          The following is a transcript of setting up and manipulating
          a RabbitMQ cluster across three machines -
          <code>rabbit1</code>, <code>rabbit2</code>,
          <code>rabbit3</code>, with two of the machines replicating
          data on ram and disk, and the other replicating data in ram
          only.
        </p>
        <p>
          We assume that the user is logged into all three machines,
          that RabbitMQ has been installed on the machines, and that
          the rabbitmq-server and rabbitmqctl scripts are in the
          user's PATH.
        </p>

        <doc:subsection name="setup">
          <doc:heading>Initial setup</doc:heading>
          <p>
            Erlang nodes use a cookie to determine whether they are
            allowed to communicate with each other - for two nodes to
            be able to communicate they must have the same cookie.
          </p>
          <p>
            The cookie is just a string of alphanumeric characters. It
            can be as long or short as you like.
          </p>
          <p>
            Erlang will automatically create a random cookie file when
            the RabbitMQ server starts up. This will be typically
            located in <code>/var/lib/rabbitmq/.erlang.cookie</code>
            on Unix systems and <code>C:\Users\<i>Current
            User</i>\.erlang.cookie</code> or <code>C:\Documents and
            Settings\<i>Current User</i>\.erlang.cookie</code> on
            Windows systems. The easiest way to proceed is to allow
            one node to create the file, and then copy it to all the
            other nodes in the cluster.
          </p>
          <p>
            As an alternative, you can insert the option "<code>-setcookie
            <i>cookie</i></code>" in the <code>erl</code> call in the
            <code>rabbitmq-server</code> and <code>rabbitmqctl</code>
            scripts.
          </p>
        </doc:subsection>

        <doc:subsection name="starting">
          <doc:heading>Starting independent nodes</doc:heading>
          <p>
            Clusters are set up by re-configuring existing RabbitMQ
            nodes into a cluster configuration. Hence the first step
            is to start RabbitMQ on all nodes in the normal way:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmq-server -detached</i>
rabbit2$ <i>rabbitmq-server -detached</i>
rabbit3$ <i>rabbitmq-server -detached</i></pre>
          <p>
            This creates three <i>independent</i> RabbitMQ brokers,
            one on each node, as confirmed by the <i>cluster_status</i>
            command:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
...done.</pre>

          <p>
            The node name of a RabbitMQ broker started from the
            <code>rabbitmq-server</code> shell script is
            <code>rabbit@<i>shorthostname</i></code>, where the short
            node name is lower-case (as in <code>rabbit@rabbit1</code>,
            above). If you use the <code>rabbitmq-server.bat</code>
            batch file on Windows, the short node name is upper-case (as
            in <code>rabbit@RABBIT1</code>). When you type node names,
            case matters, and these strings must match exactly.
          </p>
        </doc:subsection>

        <doc:subsection name="creating">
          <doc:heading>Creating the cluster</doc:heading>
          <p>
            In order to link up our three nodes in a cluster, we tell
            two of the nodes, say <code>rabbit@rabbit2</code> and
            <code>rabbit@rabbit3</code>, to join the cluster of the
            third, say <code>rabbit@rabbit1</code>.
          </p>
          <p>
            We first join <code>rabbit@rabbit2</code> as a ram node in
            a cluster with <code>rabbit@rabbit1</code> in a
            cluster. To do that, on <code>rabbit@rabbit2</code> we
            stop the RabbitMQ application, reset the node, join the
            <code>rabbit@rabbit1</code> cluster, and restart the
            RabbitMQ application.
          </p>
          <pre class="sourcecode">
rabbit2$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl cluster rabbit@rabbit1</i>
Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.
rabbit2$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit2 ...done.</pre>
          <p>
            We can see that the two nodes are joined in a cluster by
            running the <i>cluster_status</i> command on either of the nodes:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
...done.
</pre>
          <p>
            Now we join <code>rabbit@rabbit3</code> as a disk node to
            the same cluster. The steps are identical to the ones
            above, except that we list <code>rabbit@rabbit3</code> as
            a node in the <i>cluster</i> command in order to turn it
            into a disk rather than ram node.
          </p>
          <pre class="sourcecode">
rabbit3$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl cluster rabbit@rabbit1 rabbit@rabbit3</i>
Clustering node rabbit@rabbit3 with [rabbit@rabbit1, rabbit@rabbit3] ...done.
rabbit3$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit3 ...done.</pre>
          <p>
            When joining a cluster it is ok to specify nodes which are
            currently down; it is sufficient for <i>one</i> node to be
            up for the command to succeed.
          </p>
          <p>
            We can see that the three nodes are joined in a cluster by
            running the <i>cluster_status</i> command on any of the nodes:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit3]},{ram,[rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit3]},{ram,[rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit3,rabbit@rabbit1,rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit3,rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
...done.</pre>
          <p>
            By following the above steps we can add new nodes to the
            cluster at any time, while the cluster is running.
          </p>
          <p>
            <strong>Note</strong> For the <code>rabbitmqctl
            cluster</code> command to succeed, the target nodes need
            to be active.  It is possible to cluster with offline
            nodes; for this purpose, use the <code>rabbitmqctl
            force_cluster</code> command.
          </p>
        </doc:subsection>

        <doc:subsection name="change-type">
          <doc:heading>Changing node types</doc:heading>
          <p>
            We can change the type of a node from ram to disk and vice
            versa. Say we wanted to reverse the types of
            <code>rabbit@rabbit2</code> and
            <code>rabbit@rabbit3</code>, turning the former from a ram
            node into a disk node and the latter from a disk node into
            a ram node. To do that we simply stop the RabbitMQ
            application, change the type with an appropriate
            <i>cluster</i> command, and restart the application.
          </p>
          <pre class="sourcecode">
rabbit2$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl cluster rabbit@rabbit1 rabbit@rabbit2</i>
Clustering node rabbit@rabbit2 with [rabbit@rabbit1, rabbit@rabbit2] ...done.
rabbit2$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit2 ...done.
rabbit3$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl cluster rabbit@rabbit1 rabbit@rabbit2</i>
Clustering node rabbit@rabbit3 with [rabbit@rabbit1, rabbit@rabbit2] ...done.
rabbit3$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit3 ...done.</pre>
          <p>
            The significance of specifying both
            <code>rabbit@rabbit1</code> and
            <code>rabbit@rabbit2</code> as the cluster nodes for
            <code>rabbit@rabbit3</code> is that in case of failure of
            either of them, <code>rabbit@rabbit3</code> can still
            connect to the cluster when it starts, and operate
            normally. This is only important for ram nodes; disk nodes
            automatically keep track of the cluster
            configuration.
          </p>
        </doc:subsection>

        <doc:subsection name="restarting">
          <doc:heading>Restarting cluster nodes</doc:heading>

          <p>
            Nodes that have been joined to a cluster can be stopped at
            any time. It is also ok for them to crash. In both cases
            the rest of the cluster continues operating unaffected,
            and the nodes automatically "catch up" with the other
            cluster nodes when they start up again.
          </p>
          <p>
            We shut down the nodes <code>rabbit@rabbit1</code> and
            <code>rabbit@rabbit3</code> and check on the cluster
            status at each step:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl stop</i>
Stopping and halting node rabbit@rabbit1 ...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit3,rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit2,rabbit@rabbit1]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit3]}]
...done.
rabbit3$ <i>rabbitmqctl stop</i>
Stopping and halting node rabbit@rabbit3 ...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2]}]
...done.</pre>
          <p>
            Now we start the nodes again, checking on the cluster
            status as we go along:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmq-server -detached</i>
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmq-server -detached</i>
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit2,rabbit@rabbit1]},{ram,[rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
...done.</pre>
          <p>
            There are some important caveats:
          </p>
          <ul>
            <li>
              All disk nodes must be running for certain operations,
              most notably leaving a cluster, to succeed.
            </li>
            <li>
              At least one disk node should be running at all times.
            </li>
            <li>
              When all nodes in a cluster have been shut down,
              restarting any node will suspend for up to 30 seconds
              and then fail if the last disk node that was shut down
              has not been restarted yet. Since the nodes do not know
              what happened to that last node, they have to assume
              that it holds a more up-to-date version of the broker
              state. Hence, in order to preserve data integrity, they
              cannot resume operation until that node is restarted.
            </li>
          </ul>
        </doc:subsection>

        <doc:subsection name="breakup">
          <doc:heading>Breaking up a cluster</doc:heading>

          <p>
            Nodes need to be removed explicitly from a cluster when
            they are no longer meant to be part of it. This is
            particularly important in case of disk nodes since, as
            noted above, certain operations require all disk nodes to
            be up.
          </p>

          <p>
            We first remove <code>rabbit@rabbit3</code> from the
            cluster, returning it to independent operation. To do
            that, on <code>rabbit@rabbit3</code> we stop the RabbitMQ
            application, reset the node, and restart the RabbitMQ
            application.
          </p>
          <pre class="sourcecode">
rabbit3$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit3 ...done.</pre>
          <p>
            Note that it would have been equally valid to list
            <code>rabbit@rabbit3</code> as a node.
          </p>
          <p>
            Running the <i>cluster_status</i> command on the nodes confirms
            that <code>rabbit@rabbit3</code> now is no longer part of
            the cluster and operates independently:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
 {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
...done.
</pre>
          <p>
            Now we remove <code>rabbit@rabbit1</code> from the
            cluster. The steps are identical to the ones above.
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit1 ...done.
rabbit1$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit1 ...done.
rabbit1$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit1 ...done.</pre>
          <p>
            The <i>cluster_status</i> command now shows all three nodes
            operating as independent RabbitMQ brokers:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
...done.</pre>
          <p>
            Note that <code>rabbit@rabbit2</code> retains the residual
            state of the cluster, whereas <code>rabbit@rabbit1</code>
            and <code>rabbit@rabbit3</code> are freshly initialised
            RabbitMQ brokers. If we want to re-initialise
            <code>rabbit@rabbit2</code> we follow the same steps as
            for the other nodes:
          </p>
          <pre class="sourcecode">
rabbit2$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl force_reset</i>
Resetting node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl start_app</i>
Starting node rabbit@rabbit2 ...done.</pre>
          <p>
            Note that we used <em>force_reset</em> here. The reason is
            that removing a node from a cluster updates only the
            node-local configuration of the cluster, and that
            calling <em>reset</em> gets the node to connect to any of
            the other nodes that it believes are in the cluster, to
            perform some house-keeping that is necessary when leaving
            a cluster. However, at this point, there are no other
            nodes in the cluster, but <code>rabbit@rabbit2</code>
            doesn't know this. As a result, calling <em>reset</em>
            would fail, as it can't connect
            to <code>rabbit@rabbit1</code>
            or <code>rabbit@rabbit3</code>, hence the use
            of <em>force_reset</em>, in
            which <code>rabbit@rabbit2</code> does not attempt to
            contact any other nodes in the cluster. This situation
            only arises when resetting the last remaining node of a
            cluster.
          </p>

        </doc:subsection>

        <doc:subsection name="auto-config">
          <doc:heading>Auto-configuration of a cluster</doc:heading>
          <p>
            Instead of configuring clusters "on the fly" using the
            <code>cluster</code> command, clusters can also be set up
            via the RabbitMQ <code>.config</code> file; see the
            <a href="configure.html#configuration-file">configuration guide</a>
            for details. The file should set the cluster_nodes field in
            the rabbit application to a list of cluster nodes.
          </p>
          <p>
            Listing cluster nodes this way has the same effect as
            using the <code>cluster</code> command. However, the
            latter takes precedence over the former, i.e. the default
            cluster configuration is ignored subsequent to any
            successful invocation of the <code>cluster</code> command,
            until the node is <code>reset</code>.
          </p>
          <p>
            A common use of cluster configuration via the RabbitMQ
            config file is to automatically configure nodes to join a
            common cluster. For this purpose the same configuration
            can be set on all nodes, thus specifing the potential disk
            nodes for the cluster.
          </p>
          <p>
            Say we want to join our three separate nodes of our
            running example back into a single cluster, with
            <code>rabbit@rabbit1</code> and
            <code>rabbit@rabbit2</code> being the disk nodes of the
            cluster. First we reset and stop all nodes - NB: this step
            would not be necessary if this was a fresh installation of
            RabbitMQ.
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit1 ...done.
rabbit1$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit1 ...done.
rabbit1$ <i>rabbitmqctl stop</i>
Stopping and halting node rabbit@rabbit1 ...done.
rabbit2$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit2 ...done.
rabbit2$ <i>rabbitmqctl stop</i>
Stopping and halting node rabbit@rabbit2 ...done.
rabbit3$ <i>rabbitmqctl stop_app</i>
Stopping node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl reset</i>
Resetting node rabbit@rabbit3 ...done.
rabbit3$ <i>rabbitmqctl stop</i>
Stopping and halting node rabbit@rabbit3 ...done.</pre>
          <p>
            Now we set the relevant field in the config file:
          </p>
          <pre class="sourcecode">[
  ...
  {rabbit, [
        ...
  	{cluster_nodes, ['rabbit@rabbit1', 'rabbit@rabbit2']},
        ...
  ]},
  ...
].</pre>
          <p>
            For instance, if this were the only field we needed to set, we would simply create the RabbitMQ config file with the contents:
          </p>
          <pre class="sourcecode">[{rabbit, [{cluster_nodes, ['rabbit@rabbit1', 'rabbit@rabbit2']}]}].</pre>
          <p>
            (Note for Erlang programmers and the curious: this is a
            standard Erlang configuration file.  For more details, see the
            <a href="configure.html#configuration-file">configuration guide</a>
            and the
            <a href="http://www.erlang.org/doc/man/config.html">Erlang Config Man Page</a>.)
          </p>
          <p>
            We set this field in the config file on all machines.  For
            example, on a Unix system the file would typically have
            the path <code>/etc/rabbitmq/rabbitmq.config</code>. Now
            we simply start the nodes.
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmq-server -detached</i>
rabbit2$ <i>rabbitmq-server -detached</i>
rabbit3$ <i>rabbitmq-server -detached</i></pre>
          <p>
            We can see that the three nodes are joined in a cluster by
            running the <i>cluster_status</i> command on any of the nodes:
          </p>
          <pre class="sourcecode">
rabbit1$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit1 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]
...done.
rabbit2$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit2 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit3,rabbit@rabbit1,rabbit@rabbit2]}]
...done.
rabbit3$ <i>rabbitmqctl cluster_status</i>
Cluster status of node rabbit@rabbit3 ...
[{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
 {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
...done.</pre>
        <p>
          Note that, in order to remove a node from an auto-configured
          cluster, it must first be removed from the rabbitmq.config
          files of the other nodes in the cluster.  Only then, can it
          be reset safely.
        </p>
        </doc:subsection>
      </doc:section>
      <doc:section name="upgrading">
          <doc:heading>Upgrading clusters</doc:heading>
          <p>
            When upgrading from one version of RabbitMQ to another,
            RabbitMQ will automatically update its persistent data
            structures if necessary. In a cluster, this task is
            performed by the first disc node to be started (the
            "upgrader" node). Therefore when upgrading a RabbitMQ
            cluster, you should not attempt to start any RAM nodes
            first; any RAM nodes started will emit an error message
            and fail to start up.
          </p>
          <p>
            All nodes in a cluster must be running the same versions
            of Erlang and RabbitMQ, although they may have different
            plugins installed. Therefore it is necessary to stop all
            nodes in the cluster, then start all nodes when performing
            an upgrade.
          </p>
          <p>
            While not strictly necessary, it is a good idea to decide
            ahead of time which disc node will be the upgrader, stop
            that node last, and start it first. Otherwise changes to
            the cluster configuration that were made between the
            upgrader node stopping and the last node stopping will be
            lost.
          </p>
          <p>
            Automatic upgrades are only possible from RabbitMQ
            versions 2.1.1 and later. If you have an earlier cluster,
            you will need to rebuild it to upgrade.
          </p>
      </doc:section>

      <doc:section name="single-machine">
          <doc:heading>A cluster on a single machine</doc:heading>
          <p>
            Under some circumstances it can be useful to run a cluster
            of RabbitMQ nodes on a single machine. This would
            typically be useful for experimenting with clustering on a
            desktop or laptop without the overhead of starting several
            virtual machines for the cluster. The two main
            requirements for running more than one node on a single
            machine are that each node should have a unique name and
            bind to a unique port / IP address combination for each
            protocol in use.
          </p>
          <p>
	    You can start multiple nodes on the same host
	    manually by repeated invocation of
	    <code>rabbitmq-server</code> (
	    <code>rabbitmq-server.bat</code> on Windows). You must
	    ensure that for each invocation you set the environment
	    variables <code>RABBITMQ_NODENAME</code> and
	    <code>RABBITMQ_NODE_PORT</code> to suitable values.
          </p>
          <p>For example:</p>
          <pre class="sourcecode">
$ RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
$ RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=hare rabbitmq-server -detached
$ rabbitmqctl -n hare stop_app
$ rabbitmqctl -n hare reset
$ rabbitmqctl -n hare cluster rabbit@`hostname -s`
$ rabbitmqctl -n hare start_app</pre>
          <p>
            will set up a two node cluster with one disc node and one
            ram node. Note that if you have RabbitMQ opening any ports
            other than AMQP, you'll need to configure those
            not to clash as well - for example:
          </p>
          <pre class="sourcecode">
$ RABBITMQ_NODE_PORT=5672 RABBITMQ_SERVER_START_ARGS="-rabbitmq_mochiweb listeners [{mgmt,[{port,55672}]}]" RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
$ RABBITMQ_NODE_PORT=5673 RABBITMQ_SERVER_START_ARGS="-rabbitmq_mochiweb listeners [{mgmt,[{port,55673}]}]" RABBITMQ_NODENAME=hare rabbitmq-server -detached</pre>
          <p>
            will start two nodes (which can then be clustered) when
            the management plugin is installed.
          </p>
      </doc:section>
      <doc:section name="firewall">
          <doc:heading>Firewalled nodes</doc:heading>
          <p>
            The case for firewalled clustered nodes exists when nodes
            are in a data center or on a reliable network, but separated
            by firewalls. Again, clustering is not recommended over a WAN or
            when network links between nodes are unreliable.
          </p>
          <p>
            If different nodes of a cluster are in the same data center,
            but behind firewalls then additional configuration will be
            necessary to ensure inter-node communication. Erlang makes
            use of a Port Mapper Daemon (epmd) for resolution of node
            names in a cluster. Nodes must be able to reach each other
            and the port mapper daemon for clustering to work.
          </p>
          <p>
            The default epmd port is 4369, but this can be changed
            using the <span class="envvar">ERL_EPMD_PORT</span>
            environment variable. All nodes must use the same port.
            Firewalls must permit traffic on this port to pass between
            clustered nodes. For further details see the
            <a href="http://www.erlang.org/doc/man/epmd.html">Erlang epmd
            manpage</a>.
          </p>
          <p>
            Once a distributed Erlang node address has been resolved via epmd,
            other nodes will attempt to communicate directly with that address
            using the Erlang distributed node protocol. The port range for this
            communication can be configured with two parameters for the Erlang
            kernel application:
          </p>
          <ul>
            <li>inet_dist_listen_min</li>
            <li>inet_dist_listen_max</li>
          </ul>
          <p>
            Firewalls must permit traffic in this range to pass between
            clustered nodes (assuming all nodes use the same port range).
            The default port range is unrestricted.
          </p>
          <p>
            The <a href="http://www.erlang.org/doc/man/kernel_app.html">
            Erlang kernel_app manpage</a> contains more details on the
            port range that distributed Erlang nodes listen on.
            See the <a href="configure.html">configuration</a> page
            for information on how to create and edit a configuration
            file.
          </p>
      </doc:section>
  </body>
</html>
