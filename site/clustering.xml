<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!--
Copyright (c) 2007-2019 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "Licenseâ€); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
    xmlns:x="http://www.rabbitmq.com/2011/extensions"
    xmlns:xi="http://www.w3.org/2003/XInclude">
  <head>
    <title>Clustering Guide</title>
  </head>

  <body show-in-this-page="true">
      <doc:section name="overview">
        <doc:heading>Overview</doc:heading>
        <p>
          This guide covers fundamental topics related to RabbitMQ clustering:

          <ul>
            <li>How RabbitMQ nodes are identified: <a href="#node-names">node names</a></li>
            <li><a href="#cluster-formation-requirements">Requirements</a> for clustering</li>
            <li>What data is and isn't <a href="#cluster-membership">replicated between cluster nodes</a></li>
            <li>What clustering <a href="#clustering-and-clients">means for clients</a></li>
            <li><a href="#cluster-formation">How clusters are formed</a></li>
            <li>How nodes <a href="#erlang-cookie">authenticate to each other</a> (and with CLI tools)</li>
            <li><a href="#restarting">Node restarts</a> and how nodes rejoin their cluster</li>
            <li>How to <a href="#removing-nodes">remove a cluster node</a></li>
          </ul>

          and more. <a href="/cluster-formation.html">Cluster Formation and Peer Discovery</a> is a closely related guide
          that focuses on peer discovery and cluster formation automation-related topics. For queue contents
          (message) replication, see the <a href="/ha.html">Mirrored Queues</a> guide.
        </p>

        <p>
          A RabbitMQ <i>broker</i> is a logical grouping of one or
          several Erlang <i>nodes</i>, each running the RabbitMQ
          <i>application</i> and sharing users, virtual hosts,
          queues, exchanges, bindings, and runtime parameters. Sometimes we refer to the collection
          of nodes as a <i>cluster</i>.
        </p>
      </doc:section>


      <doc:section name="cluster-formation">
        <doc:heading>Cluster Formation</doc:heading>
        <doc:subsection name="cluster-formation-options">
          <doc:heading>Ways of Forming a Cluster</doc:heading>
          <p>
            A RabbitMQ cluster can formed in a number of ways:

            <ul>
              <li>Declaratively by listing cluster nodes in <a href="/configure.html">config file</a></li>
              <li>Declaratively using DNS-based discovery</li>
              <li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-aws">AWS (EC2) instance discovery</a> (via a plugin)</li>
              <li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s">Kubernetes discovery</a> (via a plugin)</li>
              <li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-consul">Consul-based discovery</a> (via a plugin)</li>
              <li>Declaratively using <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-etcd">etcd-based discovery</a> (via a plugin)</li>
              <li>Manually with <code>rabbitmqctl</code></li>
            </ul>
          </p>

          <p>
            Please refer to the <a href="/cluster-formation.html">Cluster Formation guide</a> for details.
          </p>

          <p>
            The composition of a cluster can be altered dynamically.
            All RabbitMQ brokers start out as running on a single
            node. These nodes can be joined into clusters, and
            subsequently turned back into individual brokers again.
          </p>
        </doc:subsection>

        <doc:subsection name="node-names">
          <doc:heading>Node Names (Identifiers)</doc:heading>
          <p>
            RabbitMQ nodes are identified by node names. A node name consists of two parts,
            a prefix (usually `rabbit`) and hostname. For example, `rabbit@node1.messaging.svc.local`
            is a node name with the prefix of `rabbit` and hostname of `node1.messaging.svc.local`.
          </p>

          <p>
            Node names in a cluster must be unique. If more than one node is running on a given host
            (this is usually the case in development and QA environments), they must use
            different prefixes, e.g. `rabbit1@hostname` and `rabbit2@hostname`.
          </p>

          <p>
            In a cluster, nodes identify and contact each other using node names. This means
            that the hostname part of every node name <a href="#hostname-resolution-requirement">must resolve</a>.
            <a href="/cli.html">CLI tools</a> also identify and address nodes using node names.
          </p>

          <p>
            When a node starts up, it checks whether it has been assigned a node name. This is done
            via the `RABBITMQ_NODENAME` <a href="/configure.html#supported-environment-variables">environment variable</a>.
            If no value was explicitly configured,
            the node resolves its hostname and prepends `rabbit` to it to compute its node name.
          </p>

          <p>
            If a system uses fully qualified domain names (FQDNs) for hostnames, RabbitMQ nodes
            and CLI tools must be configured to use so called long node names.
            For server nodes this is done by setting the `RABBITMQ_USE_LONGNAME` <a href="/configure.html#supported-environment-variables">environment variable</a>
            to `true`.

            For CLI tools, either `RABBITMQ_USE_LONGNAME` must be set or the `--longnames` option
            must be specified.
          </p>
        </doc:subsection>
      </doc:section>

      <doc:section name="cluster-formation-requirements">
        <doc:subsection name="hostname-resolution-requirement">
          <doc:heading>Cluster Formation Requirements</doc:heading>
          <p>
            RabbitMQ nodes address each other using domain names,
            either short or fully-qualified (FQDNs). Therefore
            hostnames of all cluster members
            must be resolvable from all cluster nodes, as well
            as machines on which command line tools such as <code>rabbitmqctl</code>
            might be used.
          </p>
          <p>
            Hostname resolution can use any of the standard OS-provided
            methods:

            <ul>
              <li>DNS records</li>
              <li>Local host files (e.g. <code>/etc/hosts</code>)</li>
            </ul>

            In more restrictive environments, where DNS record or
            hosts file modification is restricted, impossible or
            undesired, <a
            href="http://erlang.org/doc/apps/erts/inet_cfg.html">Erlang
            VM can be configured to use alternative hostname
            resolution methods</a>, such as an alternative DNS server,
            a local file, a non-standard hosts file location, or a mix
            of methods.  Those methods can work in concert with the
            standard OS hostname resolution methods.
          </p>
          <p>
            To use FQDNs, see <code>RABBITMQ_USE_LONGNAME</code> in the <a href="/configure.html#supported-environment-variables">Configuration guide</a>.
            See <a href="#node-names">Node Names</a> above.
          </p>
        </doc:subsection>

        <xi:include href="install-selinux-ports.xml.inc"/>
      </doc:section>


      <doc:section name="cluster-membership">
        <doc:heading>Nodes in a Cluster</doc:heading>

        <doc:subsection name="overview-what-is-replicated">
          <doc:heading>What is Replicated?</doc:heading>
          <p>
            All data/state required for the operation of a RabbitMQ
            broker is replicated across all nodes. An exception to this
            are message queues, which by default reside on one node,
            though they are visible and reachable from all nodes. To
            replicate queues across nodes in a cluster, see the
            documentation on <a href="ha.html">high availability</a>
            (note: this guide is a prerequisite for mirroring).
          </p>
        </doc:subsection>


        <doc:subsection name="peer-equality">
          <doc:heading>Nodes are Equal Peers</doc:heading>

          <p>
            Some distributed systems
            have leader and follower nodes. This is generally not true for RabbitMQ.
            All nodes in a RabbitMQ cluster are equal peers: there are no special nodes in RabbitMQ core.
            This topic becomes more nuanced when <a href="ha.html">queue mirroring</a> and plugins
            are taken into consideration but for most intents and purposes,
            all cluster nodes should be considered equal
          </p>

          <p>
            Many <a href="/cli.html">CLI tool</a> operations can be executed against any node.
            An <a href="/management.html">HTTP API</a> client can target any cluster node.
          </p>

          <p>
            Individual plugins can designate (elect)
            certain nodes to be "special" for a period of time. For example, <a href="/federation.html">federation links</a>
            are colocated on a particular cluster node. Should that node fail, the links will
            be restarted on a different node.
          </p>

          <p>
            In versions older than 3.6.7, <a href="/management.html">RabbitMQ management plugin</a> used
            a dedicated node for stats collection and aggregation.
          </p>
        </doc:subsection>

        <xi:include href="erlang-cookie.xml.inc"/>

        <doc:subsection name="clustering-and-clients">
          <doc:heading>Clustering and Clients</doc:heading>

          <p>
            Assuming all cluster members
            are available, a client can connect to any node and
            perform any operation. Nodes will route operations to the
            <a href="ha.html#master-migration-data-locality">queue master node</a> transparently to
            clients.
          </p>

          <p>
            With all supported messaging protocols a client is only connected to one node
            at a time.
          </p>

          <p>
            In case of a node failure, clients should be able to reconnect
            to a different node, recover their topology and continue operation. For
            this reason, most client libraries accept a list of endpoints (hostnames or IP addresses)
            as a connection option. The list of hosts will be used during initial connection
            as well as connection recovery, if the client supports it. See documentation guides
            for individual clients to learn more.
          </p>

          <p>
            There are scenarios where it may not be possible for a client to transparently continue
            operations after connecting to a different node. They usually involve
            <a href="/ha.html#non-mirrored-queue-behavior-on-node-failure">non-mirrored queues hosted on a failed node</a>.
          </p>
        </doc:subsection>


        <doc:subsection name="clustering-and-observability">
          <doc:heading>Clustering and Observability</doc:heading>

          <p>
            Client connections, channels and queues will be distributed across cluster nodes.
            Operators need to be able to inspect and <a href="/monitoring.html">monitor</a> such resources
            across all cluster nodes.
          </p>

          <p>
            RabbitMQ <a href="/cli.html">CLI tools</a> such as <code>rabbitmq-diagnostics</code> and <code>rabbitmqctl</code>
            provide commands that inspect resources and cluster-wide state. Some commands focus on the state of a single node
            (e.g. <code>rabbitmq-diagnostics environment</code> and <code>rabbitmq-diagnostics status</code>), others
            inspect cluster-wide state. Some examples of the latter include <code>rabbitmqctl list_connections</code>,
            <code>rabbitmqctl list_mqtt_connections</code>, <code>rabbitmqctl list_stomp_connections</code>, <code>rabbitmqctl list_users</code>,
            <code>rabbitmqctl list_vhosts</code> and so on.
          </p>

          <p>
            Such "cluster-wide" commands will often contact one node
            first, discover cluster members and contact them all to
            retrieve and combine their respective state. For example,
            <code>rabbitmqctl list_connections</code> will contact all
            nodes, retrieve their AMQP 0-9-1 and AMQP 1.0 connections,
            and display them all to the user.  The user doesn't have
            to manually contact all nodes. Assuming a non-changing
            state of the cluster (e.g. no connections are closed or
            opened), two CLI commands executed against two different
            nodes one after another will produce identical or
            semantically identical results. "Node-local" commands, however, will not produce
            identical results since two nodes rarely have identical state: at the very least their
            node names will be different!
          </p>

          <p>
            <a href="/management.html">Management UI</a> works similarly: a node that has to respond to an HTTP API request
            will fan out to other cluster members and aggregate their responses. In a cluster with multiple nodes that have management plugin
            enabled, the operator can use any node to access management UI. The same goes for monitoring tools that use
            the HTTP API to collect data about the state of the cluster. There is no need to issue a request to every cluster node in turn.
          </p>
        </doc:subsection>


        <doc:subsection name="clustering-dealing-with-failure">
          <doc:heading>Node Failure Handling</doc:heading>

          <p>
            RabbitMQ brokers tolerate the failure of individual
            nodes. Nodes can be started and stopped at will,
            as long as they can contact a cluster member node
            known at the time of shutdown.
          </p>

          <p>
            <a href="/ha.html">Queue mirroring</a> allows queue contents to be replicated
            across multiple cluster nodes.
          </p>

          <p>
            Non-mirrored queues can also be used in clusters. Non-mirrored queue <a href="/ha.html#non-mirrored-queue-behavior-on-node-failure">behaviour in case of node failure</a>
            depends on queue durability.
          </p>

          <p>
            RabbitMQ clustering has several modes of dealing with <a href="partitions.html">network partitions</a>,
            primarily consistency oriented. Clustering is meant to be used across LAN. It is
            not recommended to run clusters that span WAN.
            The <a href="shovel.html">Shovel</a> or
            <a href="federation.html">Federation</a>
            plugins are better solutions for connecting brokers across a
            WAN. Note that <a href="distributed.html">Shovel and Federation are not equivalent to clustering</a>.
          </p>
        </doc:subsection>


        <doc:subsection name="clustering-and-stats">
          <doc:heading>Metrics and Statistics</doc:heading>

          <p>
            Every node stores and aggregates its own metrics and stats, and provides an API for
            other nodes to access it. Some stats are cluster-wide, others are specific to individual nodes.
            Node that responds to an <a href="/management.html">HTTP API</a> request contacts its peers
            to retrieve their data and then produces an aggregated result.
          </p>

          <p>
            In versions older than 3.6.7, <a href="/management.html">RabbitMQ management plugin</a> used
            a dedicated node for stats collection and aggregation.
          </p>
        </doc:subsection>


        <doc:subsection name="cluster-node-types">
          <doc:heading>Disk and RAM Nodes</doc:heading>

          <p>
            A node can be a <em>disk node</em> or a <em>RAM node</em>.
            (<b>Note:</b> <i>disk</i> and <i>disc</i> are used
            interchangeably). RAM nodes store internal database tables
            in RAM only. This does not include messages, message store
            indices, queue indices and other node state.
          </p>

          <p>
            In the vast majority of cases you want all your nodes to be
            disk nodes; RAM nodes are a special case that can be used
            to improve the performance clusters with high queue,
            exchange, or binding churn. RAM nodes do not provide
            higher message rates. When in doubt, use
            disk nodes only.
          </p>

          <p>
            Since RAM nodes store internal database tables in RAM only, they must sync
            them from a peer node on startup. This means that a cluster must contain
            at least one disk node. It is therefore not possible to manually remove
            the last remaining disk node in a cluster.
          </p>
        </doc:subsection>
      </doc:section>


      <doc:section name="transcript">
        <doc:heading>Clustering Transcript with <code>rabbitmqctl</code></doc:heading>
        <p>
          The following is a transcript of manually setting up and manipulating
          a RabbitMQ cluster across three machines -
          <code>rabbit1</code>, <code>rabbit2</code>,
          <code>rabbit3</code>. It is recommended that the example is studied before
          <a href="/cluster-formation.html">more automation-friendly</a> cluster formation
          options are used.
        </p>
        <p>
          We assume that the user is logged into all three machines,
          that RabbitMQ has been installed on the machines, and that
          the rabbitmq-server and rabbitmqctl scripts are in the
          user's PATH.
        </p>
        <p>
          This transcript can be modified to run on a single host, as
          explained more details below.
        </p>

        <doc:subsection name="starting">
          <doc:heading>Starting Independent Nodes</doc:heading>
          <p>
            Clusters are set up by re-configuring existing RabbitMQ
            nodes into a cluster configuration. Hence the first step
            is to start RabbitMQ on all nodes in the normal way:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmq-server -detached
# on rabbit2
rabbitmq-server -detached
# on rabbit3
rabbitmq-server -detached
</pre>
          <p>
            This creates three <i>independent</i> RabbitMQ brokers,
            one on each node, as confirmed by the <i>cluster_status</i>
            command:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# => ...done.
</pre>

          <p>
            The node name of a RabbitMQ broker started from the
            <code>rabbitmq-server</code> shell script is
            <code>rabbit@<i>shorthostname</i></code>, where the short
            node name is lower-case (as in <code>rabbit@rabbit1</code>,
            above). On Windows, if <code>rabbitmq-server.bat</code>
            batch file is used, the short node name is upper-case (as
            in <code>rabbit@RABBIT1</code>). When you type node names,
            case matters, and these strings must match exactly.
          </p>
        </doc:subsection>

        <doc:subsection name="creating">
          <doc:heading>Creating a Cluster</doc:heading>
          <p>
            In order to link up our three nodes in a cluster, we tell
            two of the nodes, say <code>rabbit@rabbit2</code> and
            <code>rabbit@rabbit3</code>, to join the cluster of the
            third, say <code>rabbit@rabbit1</code>. Prior to that both
            newly joining members must be <a href="/rabbitmqctl.8.html#reset">reset</a>.
          </p>
          <p>
            We first join <code>rabbit@rabbit2</code> in a cluster
            with <code>rabbit@rabbit1</code>. To do that, on
            <code>rabbit@rabbit2</code> we stop the RabbitMQ
            application and join the <code>rabbit@rabbit1</code>
            cluster, then restart the RabbitMQ application. Note that
            a node must be <a href="/rabbitmqctl.8.html#reset">reset</a> before it can join an existing cluster.
            Resetting the node <strong>removes all resources and data that were previously
            present on that node</strong>. This means that a node cannot be made a member
            of a cluster and keep its existing data at the same time. When that's desired,
            using the <a href="/blue-green-upgrade.html">Blue/Green deployment strategy</a> or <a href="/backup.html">backup and restore</a>
            are the available options.
          </p>
          <pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit2 ...done.

rabbitmqctl reset
# => Resetting node rabbit@rabbit2 ...

rabbitmqctl join_cluster rabbit@rabbit1
# => Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.

rabbitmqctl start_app
# => Starting node rabbit@rabbit2 ...done.
</pre>
          <p>
            We can see that the two nodes are joined in a cluster by
            running the <i>cluster_status</i> command on either of the nodes:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =>  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# => ...done.
</pre>
          <p>
            Now we join <code>rabbit@rabbit3</code> to the same
            cluster. The steps are identical to the ones above, except
            this time we'll cluster to <code>rabbit2</code> to
            demonstrate that the node chosen to cluster to does not
            matter - it is enough to provide one online node and the
            node will be clustered to the cluster that the specified
            node belongs to.
          </p>
          <pre class="lang-bash">
# on rabbit3
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit3 ...done.

# on rabbit3
rabbitmqctl reset
# => Resetting node rabbit@rabbit3 ...

rabbitmqctl join_cluster rabbit@rabbit2
# => Clustering node rabbit@rabbit3 with rabbit@rabbit2 ...done.

rabbitmqctl start_app
# => Starting node rabbit@rabbit3 ...done.
</pre>
          <p>
            We can see that the three nodes are joined in a cluster by
            running the <i>cluster_status</i> command on any of the nodes:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit3,rabbit@rabbit1,rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit3,rabbit@rabbit2,rabbit@rabbit1]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# => ...done.
</pre>
          <p>
            By following the above steps we can add new nodes to the
            cluster at any time, while the cluster is running.
          </p>
        </doc:subsection>

        <doc:subsection name="restarting">
          <doc:heading>Restarting Cluster Nodes</doc:heading>

          <p>
            Nodes that have been joined to a cluster can be stopped at
            any time. They can also fail or be terminated by the OS. In all cases
            the rest of the cluster can continue operating,
            and the nodes automatically "catch up" with (sync from) the other
            cluster nodes when they start up again. Note that some <a href="/partitions.html">partition handling strategies</a>
            may work differently and affect other nodes.
          </p>
          <p>
            We shut down the nodes <code>rabbit@rabbit1</code> and
            <code>rabbit@rabbit3</code> and check on the cluster
            status at each step:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl stop
# => Stopping and halting node rabbit@rabbit1 ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit3,rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit3]}]
# => ...done.

# on rabbit3
rabbitmqctl stop
# => Stopping and halting node rabbit@rabbit3 ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit2]}]
# => ...done.
</pre>
          <p>
            Now we start the nodes again, checking on the cluster
            status as we go along:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmq-server -detached
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmq-server -detached

# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2,rabbit@rabbit3]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1,rabbit@rabbit3]}]
# => ...done.
</pre>
          <p>
            It is important to understand the process node go through when
            they are stopped and restarted.
          </p>

          <p>
            A stopping node picks an online cluster member (only disc
            nodes will be considered) to sync with after restart. Upon
            restart the node will try to contact that peer 10 times by
            default, with 30 second response timeouts.  In case the
            peer becomes available in that time interval, the node
            successfully starts, syncs what it needs from the peer and
            keeps going. If the peer does not become available, the restarted
            node will <strong>give up and voluntarily stop</strong>.
          </p>

          <p>
            When a node has no online peers during shutdown, it will start without
            attempts to sync with any known peers. It does not start as a standalone
            node, however, and peers will be able to rejoin it.
          </p>

          <p>
            A node rejoining after a node name or host name change can start as <a href="#peer-discovery-how-does-it-work">a blank node</a>
            if its data directory path changes as a result. Such nodes will fail to rejoin the cluster.
            While the node is offline, its peers can be reset or started with a blank data directory.
            In that case the recovering node will fail to rejoin its peer as well since internal data store cluster
            identity would no longer match.
          </p>

          <p>
            Consider the following scenario:

            <ol>
              <li>A cluster of 3 nodes, A, B and C is formed</li>
              <li>Node A is shut down</li>
              <li>Node B is reset</li>
              <li>Node A is started</li>
              <li>Node A tries to rejoin B but B's cluster identity has changed</li>
              <li>Node B doesn't recognise A as a known cluster member because it's been reset</li>
            </ol>

            in this case node B will reject the clustering attempt from A with an appropriate error
            message in the log:

<pre class="sourcecode">
Node 'rabbit@node1.local' thinks it's clustered with node 'rabbit@node2.local', but 'rabbit@node2.local' disagrees
</pre>

            In this case B can be reset again and then will be able to join A, or A
            can be reset and will successfully join B.
          </p>

          <p>
            When the entire cluster is brought down therefore, the last node to go down
            is the only one that didn't have any running peers at the time of shutdown.
            That node can start without contacting any peers first.
            Since nodes will try to contact a known peer for up to 5 minutes (by default), nodes
            can be restarted in any order in that period of time. In this case
            they will rejoin each other one by one successfully. This window of time
            can be adjusted using two configuration settings:

<pre class="lang-ini">
# wait for 60 seconds instead of 30
mnesia_table_loading_retry_timeout = 60000

# retry 15 times instead of 10
mnesia_table_loading_retry_limit = 15
</pre>

            By adjusting these settings and tweaking the time window in which
            known peer has to come back it is possible to account for cluster-wide
            redeployment scenarios that can be longer than 5 minutes to complete.
          </p>

          <p>
            During <a href="/upgrade.html">upgrades</a>, sometimes the last node to stop
            must be the first node to be started after the upgrade. That node will be designated to perform
            a cluster-wide schema migration that other nodes can sync from and apply when they
            rejoin.
          </p>

          <p>
            In some cases the last node to go
            offline cannot be brought back up. It can be removed from the
            cluster using the <code>forget_cluster_node</code> <a href="/cli.html">rabbitmqctl</a> command.
          </p>

          <p>
            Alternatively <code>force_boot</code> <a href="/cli.html">rabbitmqctl</a> command can be used
            on a node to make it boot without trying to sync with any
            peers (as if they were last to shut down). This is
            usually only necessary if the last node to shut down or a
            set of nodes will never be brought back online.
          </p>
        </doc:subsection>

        <doc:subsection name="removing-nodes">
          <doc:heading>Breaking Up a Cluster</doc:heading>
          <p>
            Sometimes it is necessary to remove a node from a
            cluster. The operator has to do this explicitly using a
            <code>rabbitmqctl</code> command.
          </p>
          <p>
            Some <a href="/cluster-formation.html">peer discovery mechanisms</a>
            support node health checks and forced
            removal of nodes not known to the discovery backend. That feature is
            opt-in (disabled by default).
          </p>
          <p>
            We first remove
            <code>rabbit@rabbit3</code> from the cluster, returning it to
            independent operation. To do that, on <code>rabbit@rabbit3</code> we
            stop the RabbitMQ application, reset the node, and restart the
            RabbitMQ application.
          </p>
          <pre class="lang-bash">
# on rabbit3
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit3 ...done.

rabbitmqctl reset
# => Resetting node rabbit@rabbit3 ...done.
rabbitmqctl start_app
# => Starting node rabbit@rabbit3 ...done.
</pre>
          <p>
            Note that it would have been equally valid to list
            <code>rabbit@rabbit3</code> as a node.
          </p>
          <p>
            Running the <i>cluster_status</i> command on the nodes confirms
            that <code>rabbit@rabbit3</code> now is no longer part of
            the cluster and operates independently:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# => {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1,rabbit@rabbit2]}]},
# =>  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# => ...done.
</pre>
          <p>
            We can also remove nodes remotely. This is useful, for example, when
            having to deal with an unresponsive node. We can for example remove
            <code>rabbit@rabbi1</code> from <code>rabbit@rabbit2</code>.
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit1 ...done.

# on rabbit2
rabbitmqctl forget_cluster_node rabbit@rabbit1
# => Removing node rabbit@rabbit1 from cluster ...
# => ...done.
</pre>
          <p>
            Note that <code>rabbit1</code> still thinks its clustered with
            <code>rabbit2</code>, and trying to start it will result in an
            error. We will need to reset it to be able to start it again.
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl start_app
# => Starting node rabbit@rabbit1 ...
# => Error: inconsistent_cluster: Node rabbit@rabbit1 thinks it's clustered with node rabbit@rabbit2, but rabbit@rabbit2 disagrees

rabbitmqctl reset
# => Resetting node rabbit@rabbit1 ...done.

rabbitmqctl start_app
# => Starting node rabbit@rabbit1 ...
# => ...done.
</pre>
          <p>
            The <i>cluster_status</i> command now shows all three nodes
            operating as independent RabbitMQ brokers:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1]}]},{running_nodes,[rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit2]}]},{running_nodes,[rabbit@rabbit2]}]
# => ...done.

# on rabbit3
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit3 ...
# => [{nodes,[{disc,[rabbit@rabbit3]}]},{running_nodes,[rabbit@rabbit3]}]
# => ...done.
</pre>
          <p>
            Note that <code>rabbit@rabbit2</code> retains the residual
            state of the cluster, whereas <code>rabbit@rabbit1</code>
            and <code>rabbit@rabbit3</code> are freshly initialised
            RabbitMQ brokers. If we want to re-initialise
            <code>rabbit@rabbit2</code> we follow the same steps as
            for the other nodes:
          </p>
          <pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit2 ...done.
rabbitmqctl reset
# => Resetting node rabbit@rabbit2 ...done.
rabbitmqctl start_app
# => Starting node rabbit@rabbit2 ...done.
        </pre>

          <p>
            Besides <code>rabbitmqctl forget_cluster_node</code> and the automatic cleanup of unknown nodes
            by some <a href="/cluster-formation">peer discovery</a> plugins, there are no scenarios
            in which a RabbitMQ node will permanently remove its peer node from a cluster.
          </p>
        </doc:subsection>
      </doc:section>

      <doc:section name="upgrading">
          <doc:heading>Upgrading clusters</doc:heading>
          You can find instructions for upgrading a cluster in
          <a href="/upgrade.html#rabbitmq-cluster-configuration">the upgrade guide</a>.
      </doc:section>

      <doc:section name="single-machine">
          <doc:heading>A Cluster on a Single Machine</doc:heading>
          <p>
            Under some circumstances it can be useful to run a cluster
            of RabbitMQ nodes on a single machine. This would
            typically be useful for experimenting with clustering on a
            desktop or laptop without the overhead of starting several
            virtual machines for the cluster.
          </p>
          <p>
            In order to run multiple RabbitMQ nodes on a single
            machine, it is necessary to make sure the nodes have
            distinct node names, data store locations, log file
            locations, and bind to different ports, including those
            used by plugins. See <code>RABBITMQ_NODENAME</code>,
            <code>RABBITMQ_NODE_PORT</code>, and
            <code>RABBITMQ_DIST_PORT</code> in the <a
            href="/configure.html#supported-environment-variables">Configuration
            guide</a>, as well as <code>RABBITMQ_MNESIA_DIR</code>,
            <code>RABBITMQ_CONFIG_FILE</code>, and
            <code>RABBITMQ_LOG_BASE</code> in the <a
            href="/relocate.html">File and Directory Locations
            guide</a>.
          </p>
          <p>
            You can start multiple nodes on the same host manually by
            repeated invocation of <code>rabbitmq-server</code> (
            <code>rabbitmq-server.bat</code> on Windows). For example:
          </p>
          <pre class="lang-bash">
RABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=hare rabbitmq-server -detached
rabbitmqctl -n hare stop_app
rabbitmqctl -n hare join_cluster rabbit@`hostname -s`
rabbitmqctl -n hare start_app
</pre>
          <p>
            will set up a two node cluster, both nodes as disc nodes.
            Note that if you have RabbitMQ opening any ports other
            than AMQP, you'll need to configure those not to clash as
            well. This can be done via command line:
          </p>
          <pre class="lang-bash">
RABBITMQ_NODE_PORT=5672 RABBITMQ_SERVER_START_ARGS="-rabbitmq_management listener [{port,15672}]" RABBITMQ_NODENAME=rabbit rabbitmq-server -detached
RABBITMQ_NODE_PORT=5673 RABBITMQ_SERVER_START_ARGS="-rabbitmq_management listener [{port,15673}]" RABBITMQ_NODENAME=hare rabbitmq-server -detached
</pre>
          <p>
            will start two nodes (which can then be clustered) when
            the management plugin is installed.
          </p>
      </doc:section>

      <xi:include href="clustering-hostname-issues.xml.inc"/>
      <xi:include href="clustering-firewalled-nodes.xml.inc"/>
      <xi:include href="clustering-erlang-version.xml.inc"/>

      <doc:section name="clients">
        <doc:heading>Connecting to Clusters from Clients</doc:heading>
        <p>
          A client can connect as normal to any node within a
          cluster. If that node should fail, and the rest of the
          cluster survives, then the client should notice the closed
          connection, and should be able to reconnect to some
          surviving member of the cluster. Generally, it's not
          advisable to bake in node hostnames or IP addresses into
          client applications: this introduces inflexibility and will
          require client applications to be edited, recompiled and
          redeployed should the configuration of the cluster change or
          the number of nodes in the cluster change. Instead, we
          recommend a more abstracted approach: this could be a
          dynamic DNS service which has a very short TTL
          configuration, or a plain TCP load balancer, or some sort of
          mobile IP achieved with pacemaker or similar
          technologies. In general, this aspect of managing the
          connection to nodes within a cluster is beyond the scope of
          RabbitMQ itself, and we recommend the use of other
          technologies designed specifically to solve these problems.
        </p>
      </doc:section>

      <doc:section name="ram-nodes">
        <doc:heading>Clusters with RAM nodes</doc:heading>
        <p>
          RAM nodes keep their metadata only in memory. As RAM nodes
          don't have to write to disc as much as disc nodes, they can
          perform better. However, note that since persistent queue
          data is always stored on disc, the performance improvements
          will affect only resource management (e.g. adding/removing
          queues, exchanges, or vhosts), but not publishing or
          consuming speed.
        </p>
        <p>
          RAM nodes are an advanced use case; when setting up your
          first cluster you should simply not use them. You should
          have enough disc nodes to handle your redundancy
          requirements, then if necessary add additional RAM nodes for
          scale.
        </p>
        <p>
          A cluster containing only RAM nodes is fragile; if the
          cluster stops you will not be able to start it again and
          <b>will lose all data</b>. RabbitMQ will prevent the creation of a
          RAM-node-only cluster in many situations, but it can't
          absolutely prevent it.
        </p>
        <p>
          The examples here show a cluster with one disc and one RAM
          node for simplicity only; such a cluster is a poor design
          choice.
        </p>
        <doc:subsection name="creating-ram">
          <doc:heading>Creating RAM nodes</doc:heading>
          <p>
            We can declare a node as a RAM node when it first joins
            the cluster. We do this with
            <code>rabbitmqctl join_cluster</code> as before, but passing the
            <code>--ram</code> flag:
          </p>
          <pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit2 ...done.

rabbitmqctl join_cluster --ram rabbit@rabbit1
# => Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.

rabbitmqctl start_app
# => Starting node rabbit@rabbit2 ...done.
</pre>
          <p>
            RAM nodes are shown as such in the cluster status:
          </p>
          <pre class="lang-bash">
# on rabbit1
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit1 ...
# => [{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
# =>  {running_nodes,[rabbit@rabbit2,rabbit@rabbit1]}]
# => ...done.

# on rabbit2
rabbitmqctl cluster_status
# => Cluster status of node rabbit@rabbit2 ...
# => [{nodes,[{disc,[rabbit@rabbit1]},{ram,[rabbit@rabbit2]}]},
# =>  {running_nodes,[rabbit@rabbit1,rabbit@rabbit2]}]
# => ...done.
</pre>
        </doc:subsection>

        <doc:subsection name="change-type">
          <doc:heading>Changing node types</doc:heading>
          <p>
            We can change the type of a node from ram to disc and vice
            versa. Say we wanted to reverse the types of
            <code>rabbit@rabbit2</code> and <code>rabbit@rabbit1</code>, turning
            the former from a ram node into a disc node and the latter from a
            disc node into a ram node. To do that we can use the
            <code>change_cluster_node_type</code> command. The node must be
            stopped first.
          </p>
          <pre class="lang-bash">
# on rabbit2
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit2 ...done.

rabbitmqctl change_cluster_node_type disc
# => Turning rabbit@rabbit2 into a disc node ...
# => ...done.
# => Starting node rabbit@rabbit2 ...done.

# on rabbit1
rabbitmqctl stop_app
# => Stopping node rabbit@rabbit1 ...done.

rabbitmqctl change_cluster_node_type ram
# => Turning rabbit@rabbit1 into a ram node ...

rabbitmqctl start_app
# => Starting node rabbit@rabbit1 ...done.
</pre>
        </doc:subsection>
      </doc:section>
  </body>
</html>
