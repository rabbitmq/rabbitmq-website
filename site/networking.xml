<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!--
Copyright (c) 2007-2016 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "Licenseâ€); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
      xmlns:xi="http://www.w3.org/2003/XInclude"
      xmlns:x="http://www.rabbitmq.com/2011/extensions">
  <head>
    <title>Networking and RabbitMQ</title>
  </head>
  <body>
    <doc:section name="intro">
      <doc:heading>Introduction</doc:heading>

      <doc:subsection name="intro">
        <p>
          Clients communicate with RabbitMQ over the network. All
          protocols supported by the broker are TCP-based. Both
          RabbitMQ and the operating system provide a number
          of knobs that can be tweaked. Some of them are directly
          related to TCP and IP operations, others have to do with
          application-level protocols such as TLS. This guide covers
          multiple topics related to networking in the context of
          RabbitMQ. This guide is not meant to be an extensive
          reference but rather an overview. Some tuneable parameters
          discussed are OS-specific. This guide focuses on Linux when
          covering OS-specific subjects, as it is the most common
          platform RabbitMQ is deployed on.
        </p>
        <p>
          There are several areas which can be configured or tuned:

          <ul>
            <li>Interfaces and ports</li>
            <li>TLS</li>
            <li>TCP socket settings</li>
            <li>Kernel TCP settings</li>
            <li>(AMQP 0-9-1, STOMP) Heartbeats</li>
            <li>Hostnames and DNS</li>
          </ul>

          Except for OS kernel parameters and DNS, all RabbitMQ settings
          are <a href="/configure.html">configured the same way</a>.
        </p>
        <p>
          Networking is a broad topic. There are many configuration options
          that can have positive or negative effect on certain workloads.
          As such, this guide does not try to be a complete reference but rather
          offer an index of key tunable parameters and serve as a starting
          point.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="interfaces">
      <doc:heading>Network Interfaces</doc:heading>

      <doc:subsection name="multiple-interfaces">
        <p>
          For RabbitMQ to accept client connections, it needs to bind to one or more
          interfaces and listen on (protocol-specific) ports. The interfaces are
          configured using the <code>rabbit.tcp_listeners</code> config option.
          By default, RabbitMQ will listen on port 5672 on all available interfaces.
        </p>

        <p>
          TCP listeners configure both interface and port. The following example
          demonstrates how to configure RabbitMQ on a specific IP and standard port:

<pre class="code">
listeners.tcp.1 = 192.168.1.99:5672
</pre>

Or using classic config format:

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"192.168.1.99", 5672}]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="dual-stack">
        <doc:heading>Listening on Dual Stack (Both IPv4 and IPv6) Interfaces</doc:heading>
        <p>
          The following example demonstrates how to configure RabbitMQ
          to listen on localhost only for both IPv4 and IPv6:

<pre class="code">
listeners.tcp.1 = 127.0.0.1:5672
listeners.tcp.2 = ::1:5672
</pre>

Or, in the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"127.0.0.1", 5672},
                     {"::1",       5672}]}
  ]}
].
</pre>
        </p>
        <p>
          With modern Linux kernels and Windows versions after Vista,
          when a port is specified and RabbitMQ is configured to
          listen on all IPv6 addresses but IPv4 is not disabled
          explicitly, IPv4 address will be included, so

<pre class="code">
listeners.tcp.1 = :::5672
</pre>

is equivalent to

<pre class="code">
listeners.tcp.1 = 0.0.0.0:5672
listeners.tcp.2 = :::5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"::",       5672}]}
  ]}
].
</pre>

is equivalent to

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"0.0.0.0", 5672},
                     {"::",      5672}]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="single-stack-ipv4">
        <doc:heading>Listening on IPv4 Interfaces Only</doc:heading>
        <p>
          In this example RabbitMQ will listen on an IPv4 interface only:

<pre class="code">
listeners.tcp.1 = 192.168.1.99:5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"192.168.1.99", 5672}]}
  ]}
].
</pre>
        </p>
        <p>
          Alternatively, if a single stack setup is desired, the interface can be
          configured using the <code>RABBITMQ_NODE_IP</code> environment variable.
          See our <a href="/configure.html">Configuration guide</a> for detalis.
        </p>
      </doc:subsection>

      <doc:subsection name="single-stack-ipv6">
        <doc:heading>Listening on IPv6 Interfaces Only</doc:heading>
        <p>
          In this example RabbitMQ will listen on an IPv6 interface only:

<pre class="code">
listeners.tcp.1 = fe80::2acf:e9ff:fe17:f97b:5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listeners, [{"fe80::2acf:e9ff:fe17:f97b", 5672}]}
  ]}
].
</pre>
        </p>
        <p>
          Alternatively, if a single stack setup is desired, the interface can be
          configured using the <code>RABBITMQ_NODE_IP</code> environment variable.
          See our <a href="/configure.html">Configuration guide</a> for detalis.
        </p>
      </doc:subsection>
    </doc:section>

    <xi:include href="install-selinux-ports.xml.inc" />

    <doc:section name="tls-support">
      <p>
        It is possible to encrypt connections using TLS with RabbitMQ. Authentication
        using peer certificates is also possible. Please refer to the <a href="/ssl.html">TLS/SSL guide</a>
        for more information.
      </p>
    </doc:section>

    <doc:section name="tuning-for-throughput">
      <doc:heading>Tuning for Throughput</doc:heading>

      <doc:subsection name="tuning-for-throughput-intro">
        <p>
          Tuning for throughput is a common goal. Improvements can be achieved by

          <ul>
            <li>Increasing TCP buffer sizes</li>
            <li>Ensuring Nagle's algorithm is disabled</li>
            <li>Enabling optional TCP features and extensions</li>
          </ul>

          For the latter two, see the OS-level tuning section below.

          Note that tuning for throughput will involve trade-offs. For example, increasing TCP buffer
          sizes will increase the amount of RAM used by every connection, which can be a significant
          total server RAM use increase.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-throughput-tcp-buffers">
        <doc:heading>TCP Buffer Size</doc:heading>
        <p>
          This is one of the key tunable parameters. Every TCP connection has buffers
          allocated for it. Generally speaking, the larger these buffers are, the more RAM
          is used per connection and better the throughput. On Linux, the OS will automatically
          tune TCP buffer size by default, typically settling on a value between 80 and 120 KB.
          For maximum throughput, it is possible to
          increase buffer size using the
          <code>rabbit.tcp_listen_options</code>,
          <code>rabbitmq_mqtt.tcp_listen_options</code>,
          <code>rabbitmq_amqp1_0.tcp_listen_options</code>, and
          related config keys.
        </p>
        <p>
          The following example sets TCP buffers for AMQP 0-9-1 connections to 192 KiB:

<pre class="code">
tcp_listen_options.backlog = 128
tcp_listen_options.nodelay = true
tcp_listen_options.linger.on      = true
tcp_listen_options.linger.timeout = 0
tcp_listen_options.sndbuf = 196608
tcp_listen_options.recbuf = 196608
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
  ]}
].
</pre>

          The same example for MQTT and STOMP connections:

<pre class="code">
[
  {rabbitmq_mqtt, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
                         ]},
  {rabbitmq_stomp, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
  ]}
].
</pre>

         Note that setting send and receive buffer sizes to different values is dangerous
         and is not recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-throughput-async-thread-pool">
        <doc:heading>Erlang VM I/O Thread Pool</doc:heading>

        <p>
          Erlang runtime uses a pool of threads for performing I/O
          operations asynchronously. The size of the pool is <a href="/configure.html">configured</a> via
          the <code>+A</code> VM command line flag, e.g. <code>+A 128</code>. We highly recommend
          overriding the flag using the `RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS` environment
          variable:

<pre class="code">
RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="+A 128"
</pre>

          Default value in recent RabbitMQ releases is 128 (30 previously). Nodes that have 8 or more cores available are recommended
          to use values higher than 96, that is, 12 or more I/O threads for every core available.
          Note that higher values do not necessarily mean better throughput or lower CPU
          burn due to waiting on I/O.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="tuning-for-large-number-of-connections">
      <doc:heading>Tuning for a Large Number of Connections</doc:heading>

      <doc:subsection name="tuning-for-large-number-of-connections-intro">
        <p>
          Some workloads, often referred to as "the Internet of
          Things", assume a large number of client connections per
          node, and a relatively low volume of traffic from each node.
          One such workload is sensor networks: there can be hundreds
          of thousands or millions of sensors deployed, each emitting
          data every several minutes. Optimising for the maximum
          number of concurrent clients can be more important than for
          total throughput.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-limitations">
        <p>
          Several factors can limit how many concurrent connections a single node can support:

          <ul>
            <li>Maximum number of open file handles (including sockets) as well as other kernel-enforced resource limits</li>
            <li>Amount of RAM used by each connection</li>
            <li>Amount of CPU resources used by each connection</li>
            <li>Maximum number of Erlang processes the VM is configured to allow</li>
          </ul>
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-open-file-handles-limit">
        <doc:heading>Open File Handles Limit</doc:heading>
        <p>
          Most operating systems limit the number of file handles that
          can be opened at the same time. <a href="http://docs.basho.com/riak/latest/ops/tuning/linux/">How
          the limit is configured</a> varies from OS to OS.

          When optimising for the number of concurrent connections,
          making sure your system has enough file descriptors to
          support not only client connections but also files the node
          may use. To calculate a ballpark limit, multiply the number
          of connections per node by 1.5. For example, to support 100,000
          connections, set the limit to 150,000.

          Increasing the limit slightly increases the amount of
          RAM idle machine uses but this is a reasonable trade-off.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-tcp-buffer-size">
        <doc:heading>TCP Buffer Size</doc:heading>
        <p>
          See the section above for an overview. It is possible to
          decrease buffer size using the
          <code>rabbit.tcp_listen_options</code>,
          <code>rabbitmq_mqtt.tcp_listen_options</code>,
          <code>rabbitmq_amqp1_0.tcp_listen_options</code>, and
          related config keys to reduce the amount of RAM by the
          server used per connection. This is often necessary in
          environments where the number of concurrent connections
          sustained per node is more important than throughput.
        </p>
        <p>
          The following example sets TCP buffers for AMQP 0-9-1 connections to 32 KiB:

<pre class="code">
tcp_lsiten_options.backlog = 128
tcp_lsiten_options.nodelay = true
tcp_listen_options.linger.on      = true
tcp_listen_options.linger.timeout = 0
tcp_lsiten_options.sndbuf  = 32768
tcp_lsiten_options.recbuf  = 32768
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
  ]}
].
</pre>

          The same example for MQTT and STOMP connections:

<pre class="code">
[
  {rabbitmq_mqtt, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
                         ]},
  {rabbitmq_stomp, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
  ]}
].
</pre>

         Note that lower TCP buffer sizes will result in a significant throughput drop,
         so an optimal value between throughput and per-connection RAM use needs to be
         found for every workload.

         Setting send and receive buffer sizes to different values is dangerous
         and is not recommended. Values lower than 8 KiB are not recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-nodelay">
        <doc:heading>Nagle's Algorithm ("nodelay")</doc:heading>
        <p>
          Disabling <a
          href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's
          algorithm</a> is primarily useful for reducing latency but
          can also improve
          throughput. <code>kernel.inet_default_connect_options</code>
          and <code>kernel.inet_default_listen_options</code> must
          include <code>{nodelay, true}</code> to disable Nagle's
          algorithm for inter-node connections.  When configuring
          sockets that serve client connections,
          <code>rabbit.tcp_listen_options</code> must include the same
          option. This is the default.

          The following example demonstrates that:

In <code>rabbitmq.conf</code>

<pre class="code">
    tcp_listen_options.backlog = 4096
    tcp_listen_options.nodelay = true
</pre>

together with the following bits in the <a href="/configure.html#advanced-config-file">advanced config file</a>:

<pre class="code">
[
  {kernel, [
    {inet_default_connect_options, [{nodelay, true}]},
    {inet_default_listen_options,  [{nodelay, true}]}
  ]}].
</pre>

When using the <a href="/configure.html#erlang-term-config-file">classic config format</a>,
everything is configured in a single file:

<pre class="code">
[
  {kernel, [
    {inet_default_connect_options, [{nodelay, true}]},
    {inet_default_listen_options,  [{nodelay, true}]}
  ]},
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       4096},
                          {nodelay,       true},
                          {linger,        {true, 0}}
                         ]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-async-thread-pool">
        <doc:heading>Erlang VM I/O Thread Pool Tuning</doc:heading>
        <p>
          Adequate Erlang VM I/O thread pool size is also important when tuning for a large number of
          concurrent connections. See the section above.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-connection-backlog">
        <doc:heading>Connection Backlog</doc:heading>
        <p>
          With a low number of clients, new connection rate is very unevenly distributed
          but is also small enough to not make much difference. When the number reaches tens of thousands
          or more, it is important to make sure that the server can accept inbound connections.
          Unaccepted TCP connections are put into a queue with bounded length. This length has to be
          sufficient to account for peak load hours and possible spikes, for instance, when many clients
          disconnect due to a network interruption or choose to reconnect.
          This is configured using the <code>rabbit.tcp_listen_options.backlog</code>
          option:

<pre class="code">
tcp_listen_options.backlog = 4096
tcp_listen_options.nodelay = true
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="code">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       4096},
                          {nodelay,       true},
                          {linger,        {true, 0}},
                          {exit_on_close, false}
                         ]}
  ]}
].
</pre>

           Default value is 128. When pending connection queue length grows beyond this value,
           connections will be rejected by the operating system. See also <code>net.core.somaxconn</code>
           in the kernel tuning section.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="os-tuning">
      <doc:heading>OS Level Tuning</doc:heading>

      <doc:subsection name="os-tuning-intro">
        <p>
          Operating system settings can affect operation of RabbitMQ.
          Some are directly related to networking (e.g. TCP settings), others
          affect TCP sockets as well as other things (e.g. open file handles limit).

          Understanding these limits is important, as they may change depending on
          the workload.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-important-options">
        <p>
          A few important configurable kernel options include (for IPv4):

          <dl>
            <dt><code>fs.file-max</code></dt>
            <dd>
              Max number of files the kernel will allocate. Limits and current value
              can be inspected using <code>/proc/sys/fs/file-nr</code>.
            </dd>

            <dt><code>net.ipv4.ip_local_port_range</code></dt>
            <dd>
              Local IP port range, define as a pair of values. The range must provide enough
              entries for the peak number of concurrent connections.
            </dd>

            <dt><code>net.ipv4.tcp_tw_reuse</code></dt>
            <dd>
              When enabled, allows the kernel to reuse sockets in <code>TIME_WAIT</code>
              state when it's safe to do so. See <a href="http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html">Coping with the TCP TIME_WAIT connections on busy servers</a> for details.
              This option is dangerous when used behind NAT.
            </dd>

            <dt><code>net.ipv4.tcp_fin_timeout</code></dt>
            <dd>
              Lowering this value to 5-10 reduces the amount of time closed connections
              will stay in the TIME_WAIT state. Recommended for cases when a large
              number of concurrent connections is expected.
            </dd>

            <dt><code>net.core.somaxconn</code></dt>
            <dd>
              Size of the listen queue (how many connections are in
              the process of being established at the same time).
              Default is 128. Increase to 4096 or higher to support
              inbound connection bursts, e.g. when clients reconnect
              en masse.
            </dd>

            <dt><code>net.ipv4.tcp_max_syn_backlog</code></dt>
            <dd>
              Maximum number of remembered connection requests which
              did not receive an acknowledgment yet from
              connecting client. Default is 128, max value is 65535. 4096 and 8192 are
              recommended starting values when optimising for throughput.
            </dd>

            <dt><code>net.ipv4.tcp_keepalive_*</code></dt>
            <dd>
              <code>net.ipv4.tcp_keepalive_time</code>, <code>net.ipv4.tcp_keepalive_intvl</code>,
              and <code>net.ipv4.tcp_keepalive_probes</code> configure TCP keepalive.

              AMQP 0-9-1 and STOMP have <a href="/heartbeats.html">Heartbeats</a> which partially
              undo its effect, namely that it can take minutes to detect an unresponsive peer,
              e.g. in case of a hardware or power failure.

              When enabling TCP keepalive, we recommend setting heartbeat timeout to 8-20 seconds.
            </dd>

            <dt><code>net.ipv4.conf.default.rp_filter</code></dt>
            <dd>
              Enabled reverse path filtering. If <a href="http://en.wikipedia.org/wiki/IP_address_spoofing">IP address spoofing</a>
              is not a concern for your system, disable it.
            </dd>
          </dl>


          Note that default values for these vary between Linux kernel releases and distributions.
          Using a recent kernel (3.9 or later) is recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-important-how-to-configure">
        <p>
          Kernel parameter tuning differs from OS to OS. This guide focuses on Linux.
          To configure a kernel parameter interactively, use <code>sysctl -w</code> (requires superuser
          privileges), for example:

<pre class="code">
sysctl -w fs.file-max 200000
</pre>

          To make the changes permanent (stick between reboots), they need to be added to
          <code>/etc/sysctl.conf</code>. See <a href="http://man7.org/linux/man-pages/man8/sysctl.8.html">sysctl(8)</a>
          and <a href="http://man7.org/linux/man-pages/man5/sysctl.conf.5.html">sysctl.conf(5)</a>
          for more details.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-outro">
        <p>
          TCP stack tuning is a broad topic that is covered in much detail elsewhere:

          <ul>
            <li><a href="http://www.psc.edu/index.php/networking/641-tcp-tune">Enabling High Performance Data Transfers</a></li>
            <li><a href="https://fasterdata.es.net/network-tuning/">Network Tuning Guide</a></li>
          </ul>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="socket-gen-tcp-options">
      <doc:heading>TCP Socket Options</doc:heading>

      <doc:subsection name="socket-gen-tcp-options-common">
        <doc:heading>Common Options</doc:heading>
        <dl>
          <dt><code>tcp_listen_options.nodelay</code></dt>
          <dd>
            When set to <code>true</code>, disables
            <a href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's algorithm</a>.
            Default is true. Highly recommended for most users.
          </dd>
          <dt><code>tcp_listen_options.sndbuf</code></dt>
          <dd>
            See TCP buffers discussion earlier in this guide. Default value is
            automatically tuned by the OS, typically in the 88 KiB to 128 KiB range on
            modern Linux versions. Increasing buffer size improves consumer throughput
            and RAM use for every connection. Decreasing has the opposite effect.
          </dd>
          <dt><code>tcp_listen_options.recbuf</code></dt>
          <dd>
            See TCP buffers discussion earlier in this guide. Default value effects
            are similar to that of <code>rabbit.tcp_listen_options.sndbuf</code> but
            for publishers and protocol operations in general.
          </dd>
          <dt><code>tcp_listen_options.backlog</code></dt>
          <dd>
            Maximum size of the unaccepted TCP connections queue. When this size
            is reached, new connections will be rejected. Set to 4096 or higher for
            environments with thousands of concurrent connections and possible bulk client
            reconnections.
          </dd>
          <dt><code>tcp_listen_options.keepalive</code></dt>
          <dd>
            When set to <code>true</code>, enables TCP keepalives (see above).
            Default is <code>false</code>. Makes sense for environments where
            connections can go idle for a long time (at least 10 minutes),
            although using <a href="/heartbeats.html">heartbeats</a> is still recommended over
            this option.
          </dd>
        </dl>

      </doc:subsection>

      <doc:subsection name="socket-gen-tcp-options-defaults">
        <doc:heading>Defaults</doc:heading>

        <p>
          Below is the default TCP socket option configuration used by RabbitMQ:

          <ul>
            <li>TCP connection backlog is limited to 128 connections</li>
            <li>Nagle's algorithm is disabled</li>
            <li>Server socket lingering is enabled with the timeout of 0</li>
          </ul>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="heartbeats">
      <doc:heading>Heartbeats</doc:heading>

      <p>
        Some protocols supported by RabbitMQ, including AMQP 0-9-1, support <em>heartbeats</em>, a way to detect dead
        TCP peers quicker. Please refer to the <a href="/heartbeats.html">Heartbeats guide</a>
        for more information.
      </p>
    </doc:section>

    <doc:section name="nettick">
      <doc:heading>Net Tick Time</doc:heading>

      <p>
        <a href="/heartbeats.html">Heartbeats</a> are used to detect peer or connection failure
        between clients and RabbitMQ nodes. <a href="/net_ticktime.html">net_ticktime</a> serves
        the same purpose but for cluster node communication. Values lower than 5 (seconds)
        may result in false positive and are not recommended.
      </p>
    </doc:section>

    <doc:section name="nettick">
      <doc:heading>Connection Handshake Timeout</doc:heading>

      <p>
        RabbitMQ has a timeout for connection handshake, 10 seconds by
        default. When clients run in heavily constrained environments,
        it may be necessary to increase the timeout. This can be done via
        the <code>rabbit.handshake_timeout</code> (in milliseconds):
<pre class="code">
    handshake_timeout = 20000
</pre>

Using the classic config format:

<pre class="code">
[
  {rabbit, [
    %% 20 seconds
    {handshake_timeout, 20000}
  ]}
].
</pre>

        It should be pointed out that this is only necessary with very constrained
        clients and networks. Handshake timeouts in other circumstances indicate
        a problem elsewhere.
      </p>

      <doc:subsection name="tls-handshake">
        <doc:heading>TLS/SSL Handshake</doc:heading>

        <p>
          If TLS/SSL is enabled, it may necessary to increase also the TLS/SSL
          handshake timeout. This can be done via
          the <code>rabbit.ssl_handshake_timeout</code> (in milliseconds):

<pre class="code">
    ssl_handshake_timeout = 10000
</pre>

Using the classic config format:

<pre class="code">
[
  {rabbit, [
    %% 10 seconds
    {ssl_handshake_timeout, 10000}
  ]}
].
</pre>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="dns">
      <doc:heading>Hostname Resolution and DNS</doc:heading>

      <p>
        In many cases, RabbitMQ relies on the Erlang runtime for inter-node communication (including
        tools such as <code>rabbitmqctl</code>, <code>rabbitmq-plugins</code>, etc). Client libraries
        also perform hostname resolution when connecting to RabbitMQ nodes. This section briefly
        covers most common issues associated with that.
      </p>


      <doc:subsection name="dns-resolution-by-clients">
        <doc:heading>Performed by Client Libraries</doc:heading>

        <p>
          If a client library is configured to connect to a hostname, it performs
          hostname resolution. Depending on DNS and local resolver (<code>/etc/hosts</code>
          and similar) configuration, this can take some time. Incorrect configuration
          may lead to resolution timeouts, e.g. when trying to resolve a local hostname
          such as <code>my-dev-machine</code>, over DNS. As a result, client connections
          can take a long time (from tens of seconds to a few minutes).
        </p>
      </doc:subsection>

      <doc:subsection name="dns-resolution-by-nodes">
        <doc:heading>Short and Fully-qualified RabbitMQ Node Names</doc:heading>

        <p>
          RabbitMQ relies on the Erlang runtime for inter-node
          communication. Erlang nodes include a hostname, either short
          (<code>rmq1</code>) or fully-qualified
          (<code>rmq1.dev.megacorp.local</code>). Mixing short and
          fully-qualified hostnames is not allowed by the
          runtime. Every node in a cluster must be able to resolve
          every other node's hostname, short or fully-qualified.

          By default RabbitMQ will use short hostnames. Set the
          <code>RABBITMQ_USE_LONGNAME</code> environment variable to
          make RabbitMQ nodes use fully-qualified names,
          e.g. <code>rmq1.dev.megacorp.local</code>.
        </p>
      </doc:subsection>

      <doc:subsection name="dns-reverse-dns-lookups">
        <doc:heading>Reverse DNS Lookups</doc:heading>

        <p>
          If the <code>rabbit.reverse_dns_lookups</code> configuration option is set to <code>true</code>,
          RabbitMQ will perform reverse DNS lookups for client IP addresses and list hostnames
          in connection information (e.g. in the <a href="/management.html">Management UI</a>).
        </p>
      </doc:subsection>
    </doc:section>
  </body>
</html>
