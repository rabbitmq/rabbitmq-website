<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!--
Copyright (c) 2007-2018 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "License”); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
      xmlns:xi="http://www.w3.org/2003/XInclude"
      xmlns:x="http://www.rabbitmq.com/2011/extensions">
  <head>
    <title>Networking and RabbitMQ</title>
  </head>
  <body>
    <doc:section name="intro">
      <doc:heading>Introduction</doc:heading>

      <doc:subsection name="intro">
        <p>
          Clients communicate with RabbitMQ over the network. All
          protocols supported by the broker are TCP-based. Both
          RabbitMQ and the operating system provide a number
          of knobs that can be tweaked. Some of them are directly
          related to TCP and IP operations, others have to do with
          application-level protocols such as TLS. This guide covers
          multiple topics related to networking in the context of
          RabbitMQ. This guide is not meant to be an extensive
          reference but rather an overview. Some tuneable parameters
          discussed are OS-specific. This guide focuses on Linux when
          covering OS-specific subjects, as it is the most common
          platform RabbitMQ is deployed on.
        </p>
        <p>
          There are several areas which can be configured or tuned:

          <ul>
            <li>Interfaces and ports</li>
            <li>TLS</li>
            <li>TCP buffer size (directly translate into per-connection <a href="/memory-use.html">memory use</a>)</li>
            <li>Other TCP socket settings</li>
            <li>Kernel TCP settings (e.g. TCP keepalives)</li>
            <li>(AMQP 0-9-1, STOMP) Heartbeats, known as keepalives in MQTT</li>
            <li>Hostnames, hostname resolution and DNS</li>
          </ul>

          Except for OS kernel parameters and DNS, all RabbitMQ settings
          are <a href="/configure.html">configured via RabbitMQ configuration file(s)</a>.
        </p>
        <p>
          Networking is a broad topic. There are many configuration options
          that can have positive or negative effect on certain workloads.
          As such, this guide does not try to be a complete reference but rather
          offer an index of key tunable parameters and serve as a starting
          point.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="interfaces">
      <doc:heading>Network Interfaces</doc:heading>

      <doc:subsection name="multiple-interfaces">
        <p>
          For RabbitMQ to accept client connections, it needs to bind to one or more
          interfaces and listen on (protocol-specific) ports. The interfaces are
          configured using the <code>rabbit.tcp_listeners</code> config option.
          By default, RabbitMQ will listen on port 5672 on all available interfaces.
        </p>

        <p>
          TCP listeners configure both interface and port. The following example
          demonstrates how to configure RabbitMQ on a specific IP and standard port:

<pre class="sourcecode ini">
listeners.tcp.1 = 192.168.1.99:5672
</pre>

Or using classic config format:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"192.168.1.99", 5672}]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="dual-stack">
        <doc:heading>Listening on Dual Stack (Both IPv4 and IPv6) Interfaces</doc:heading>
        <p>
          The following example demonstrates how to configure RabbitMQ
          to listen on localhost only for both IPv4 and IPv6:

<pre class="sourcecode ini">
listeners.tcp.1 = 127.0.0.1:5672
listeners.tcp.2 = ::1:5672
</pre>

Or, in the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"127.0.0.1", 5672},
                     {"::1",       5672}]}
  ]}
].
</pre>
        </p>
        <p>
          With modern Linux kernels and Windows versions after Vista,
          when a port is specified and RabbitMQ is configured to
          listen on all IPv6 addresses but IPv4 is not disabled
          explicitly, IPv4 address will be included, so

<pre class="sourcecode ini">
listeners.tcp.1 = :::5672
</pre>

is equivalent to

<pre class="sourcecode ini">
listeners.tcp.1 = 0.0.0.0:5672
listeners.tcp.2 = :::5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"::",       5672}]}
  ]}
].
</pre>

is equivalent to

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"0.0.0.0", 5672},
                     {"::",      5672}]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="single-stack-ipv4">
        <doc:heading>Listening on IPv4 Interfaces Only</doc:heading>
        <p>
          In this example RabbitMQ will listen on an IPv4 interface only:

<pre class="sourcecode ini">
listeners.tcp.1 = 192.168.1.99:5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"192.168.1.99", 5672}]}
  ]}
].
</pre>
        </p>
        <p>
          Alternatively, if a single stack setup is desired, the interface can be
          configured using the <code>RABBITMQ_NODE_IP</code> environment variable.
          See our <a href="/configure.html">Configuration guide</a> for detalis.
        </p>
      </doc:subsection>

      <doc:subsection name="single-stack-ipv6">
        <doc:heading>Listening on IPv6 Interfaces Only</doc:heading>
        <p>
          In this example RabbitMQ will listen on an IPv6 interface only:

<pre class="sourcecode ini">
listeners.tcp.1 = fe80::2acf:e9ff:fe17:f97b:5672
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listeners, [{"fe80::2acf:e9ff:fe17:f97b", 5672}]}
  ]}
].
</pre>
        </p>
        <p>
          Alternatively, if a single stack setup is desired, the interface can be
          configured using the <code>RABBITMQ_NODE_IP</code> environment variable.
          See our <a href="/configure.html">Configuration guide</a> for detalis.
        </p>
      </doc:subsection>

      <doc:subsection name="proxy-protocol">
        <doc:heading>Proxy Protocol</doc:heading>
    <p>
        RabbitMQ supports the
        <a href="http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">proxy protocol</a>.
        The protocol makes servers such as RabbitMQ aware of the actual client IP address
        when connections go over a proxy (e.g. HAproxy or AWS ELB).
        This makes it easier for the operator to inspect connection origins in the management UI
        or CLI tools.
    </p>
    <p>
        The protocol spec
        dictates that either it must be applied to all connections or none of them for
        security reasons, this feature is disabled by default and needs to be enabled
        for individual protocols supported by RabbitMQ. To enable it for AMQP 0-9-1 and AMQP 1.0 clients:

<pre class="sourcecode ini">
proxy_protocol = true
</pre>

        Or, using the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {proxy_protocol, true}
  ]}
].
</pre>
    </p>
    <p>
      When proxy protocol is enabled, clients won't be able to connect
      to RabbitMQ directly unless they themselves support the protocol.
      Therefore, when this option is enabled, all client connections must go through
      a proxy that also supports the protocol, e.g. <a href="http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">HAproxy</a>
      or <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html">AWS ELB</a>.
    </p>
    <p>
      When proxy protocol is enabled and connections go through a compatible proxy, no action
      or modifications are required from client libraries. The communication is entirely
      transparent to them.
    </p>
    <p>
        <a href="stomp.html#proxy-protocol">STOMP</a> and
        <a href="mqtt.html#proxy-protocol">MQTT</a>
        have their own settings that enable support for the proxy protocol.
    </p>
      </doc:subsection>
    </doc:section>

    <xi:include href="install-selinux-ports.xml.inc" />

    <doc:section name="epmd-inet-dist-port-range">
      <doc:heading>EPMD and Inter-node Communication Port(s)</doc:heading>

      <p>
          Erlang makes use of a Port Mapper Daemon (epmd) for
          resolution of node names in a cluster. The default epmd
          port is 4369, but this can be changed using the <span
          class="envvar">ERL_EPMD_PORT</span> environment
          variable. All nodes must use the same port. For further
          details see the <a
          href="http://www.erlang.org/doc/man/epmd.html">Erlang epmd
          manpage</a>.
      </p>
      <p>
          Once a distributed Erlang node address has been resolved
          via epmd, other nodes will attempt to communicate directly
          with that address using the Erlang distribution
          protocol.
          See the following section for details.
      </p>
      <p>
        RabbitMQ nodes communicate with CLI tools and other nodes using a port known as
        the <em>distribution port</em>. It is dynamically allocated from a range of values.
        For RabbitMQ, the default range is limited to a single value computed as
        <span class="envvar">RABBITMQ_NODE_PORT</span> (AMQP port) + 20000, which results
        in using port 25672. This single port can be <a href="configure.html">configured</a>
        using the <span class="envvar">RABBITMQ_DIST_PORT</span> environment variable.
      </p>
      <p>
        RabbitMQ <a href="/cli.html">command line tools</a>
        also use a range of ports. The default range is computed by taking the RabbitMQ
        distribution port value and adding 10000 to it. The next 10 ports are also part
        of this range. Thus, by default, this range is 35672 through 35682. This range
        can be configured using the <span class="envvar">RABBITMQ_CTL_DIST_PORT_MIN</span>
        and <span class="envvar">RABBITMQ_CTL_DIST_PORT_MAX</span> environment variables.
        Note that limiting the range to a single port will prevent more than one CLI
        tool from running concurrently on the same host and may affect CLI commands
        that require parallel connections to multiple cluster nodes. A port range of 10
        is therefore a recommended value.
      </p>
      <p>
        The range used by RabbitMQ can also be controlled via two configuration keys:

        <ul>
          <li><code>kernel.inet_dist_listen_min</code></li>
          <li><code>kernel.inet_dist_listen_max</code></li>
        </ul>

        They define the range's lower and upper bounds, inclusive.
      </p>
      <p>
        The example below uses a range with a single port but a value different from default:

<pre class="sourcecode erlang">
[
  {kernel, [
    {inet_dist_listen_min, 33672},
    {inet_dist_listen_max, 33672}
  ]},
  {rabbit, [
    ...
  ]}
].
</pre>
      </p>
      <p>
        To verify what port is used by a node for inter-node and CLI tool communication,
        run

<pre class="sourcecode bash">
epmd -names
</pre>

        on that node's host. It will produce output that looks like this:

<pre class="sourcecode ini">
epmd: up and running on port 4369 with data:
name rabbit at port 25672
</pre>
      </p>
    </doc:section>

    <doc:section name="tls-support">
      <doc:heading>TLS (SSL) Support</doc:heading>
      <p>
        It is possible to encrypt connections using TLS with RabbitMQ. Authentication
        using peer certificates is also possible. Please refer to the <a href="/ssl.html">TLS/SSL guide</a>
        for more information.
      </p>
    </doc:section>

    <doc:section name="tuning-for-throughput">
      <doc:heading>Tuning for Throughput</doc:heading>

      <doc:subsection name="tuning-for-throughput-intro">
        <p>
          Tuning for throughput is a common goal. Improvements can be achieved by

          <ul>
            <li>Increasing TCP buffer sizes</li>
            <li>Ensuring Nagle's algorithm is disabled</li>
            <li>Enabling optional TCP features and extensions</li>
          </ul>

          For the latter two, see the OS-level tuning section below.

          Note that tuning for throughput will involve trade-offs. For example, increasing TCP buffer
          sizes will increase the amount of RAM used by every connection, which can be a significant
          total server RAM use increase.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-throughput-tcp-buffers">
        <doc:heading>TCP Buffer Size</doc:heading>
        <p>
          This is one of the key tunable parameters. Every TCP connection has buffers
          allocated for it. Generally speaking, the larger these buffers are, the more RAM
          is used per connection and better the throughput. On Linux, the OS will automatically
          tune TCP buffer size by default, typically settling on a value between 80 and 120 KB.
          For maximum throughput, it is possible to
          increase buffer size using the
          <code>rabbit.tcp_listen_options</code>,
          <code>rabbitmq_mqtt.tcp_listen_options</code>,
          <code>rabbitmq_amqp1_0.tcp_listen_options</code>, and
          related config keys.
        </p>
        <p>
          The following example sets TCP buffers for AMQP 0-9-1 connections to 192 KiB:

<pre class="sourcecode ini">
tcp_listen_options.backlog = 128
tcp_listen_options.nodelay = true
tcp_listen_options.linger.on      = true
tcp_listen_options.linger.timeout = 0
tcp_listen_options.sndbuf = 196608
tcp_listen_options.recbuf = 196608
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false},
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
  ]}
].
</pre>

          The same example for MQTT and STOMP connections:

<pre class="sourcecode erlang">
[
  {rabbitmq_mqtt, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false},
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
                         ]},
  {rabbitmq_stomp, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false}
                          {sndbuf,        196608},
                          {recbuf,        196608}
                         ]}
  ]}
].
</pre>

         Note that setting send and receive buffer sizes to different values is dangerous
         and is not recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-throughput-async-thread-pool">
        <doc:heading>Erlang VM I/O Thread Pool</doc:heading>

        <p>
          Erlang runtime uses a pool of threads for performing I/O
          operations asynchronously. The size of the pool is <a href="/configure.html">configured</a> via
          the <code>+A</code> VM command line flag, e.g. <code>+A 128</code>. We highly recommend
          overriding the flag using the `RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS` environment
          variable:

<pre class="sourcecode bash">
RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="+A 128"
</pre>

          Default value in recent RabbitMQ releases is 128 (30 previously). Nodes that have 8 or more cores available are recommended
          to use values higher than 96, that is, 12 or more I/O threads for every core available.
          Note that higher values do not necessarily mean better throughput or lower CPU
          burn due to waiting on I/O.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="tuning-for-large-number-of-connections">
      <doc:heading>Tuning for a Large Number of Connections</doc:heading>

      <doc:subsection name="tuning-for-large-number-of-connections-intro">
        <p>
          Some workloads, often referred to as "the Internet of
          Things", assume a large number of client connections per
          node, and a relatively low volume of traffic from each node.
          One such workload is sensor networks: there can be hundreds
          of thousands or millions of sensors deployed, each emitting
          data every several minutes. Optimising for the maximum
          number of concurrent clients can be more important than for
          total throughput.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-limitations">
        <p>
          Several factors can limit how many concurrent connections a single node can support:

          <ul>
            <li>Maximum number of open file handles (including sockets) as well as other kernel-enforced resource limits</li>
            <li>Amount of RAM used by each connection</li>
            <li>Amount of CPU resources used by each connection</li>
            <li>Maximum number of Erlang processes the VM is configured to allow</li>
          </ul>
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-open-file-handles-limit">
        <doc:heading>Open File Handle Limit</doc:heading>
        <p>
          Most operating systems limit the number of file handles that
          can be opened at the same time. When an OS process (such as RabbitMQ's Erlang VM) reaches
          the limit, it won't be able to open any new files or accept any more
          TCP connections.
        </p>

        <p>
          How the limit is configured <a href="https://github.com/basho/basho_docs/blob/master/content/riak/kv/2.2.3/using/performance/open-files-limit.md">varies from OS to OS</a> and distribution to distribution, e.g. depending on whether systemd is used.
          For Linux, Controlling System Limits on Linux
          in our <a href="/install-debian.html#kernel-resource-limits">Debian</a> and <a href="/install-rpm.html#kernel-resource-limits">RPM</a>
          installation guides provides. Linux kernel limit management is covered by many resources on the Web,
          including the <a href="https://ro-che.info/articles/2017-03-26-increase-open-files-limit">open file handle limit</a>.
        </p>
        <p>
          MacOS uses a <a href="https://superuser.com/questions/433746/is-there-a-fix-for-the-too-many-open-files-in-system-error-on-os-x-10-7-1">similar system</a>.
        </p>
        <p>
          On Windows, the limit for the Erlang runtime is controlled using the <code>ERL_MAX_PORTS</code> environment variable.
        </p>

        <p>
          When optimising for the number of concurrent connections,
          making sure your system has enough file descriptors to
          support not only client connections but also files the node
          may use. To calculate a ballpark limit, multiply the number
          of connections per node by 1.5. For example, to support 100,000
          connections, set the limit to 150,000.

          Increasing the limit slightly increases the amount of
          RAM idle machine uses but this is a reasonable trade-off.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-tcp-buffer-size">
        <doc:heading>Per Connection Memory Consumption: TCP Buffer Size</doc:heading>
        <p>
          See the section above for an overview. It is possible to
          decrease buffer size using the
          <code>rabbit.tcp_listen_options</code>,
          <code>rabbitmq_mqtt.tcp_listen_options</code>,
          <code>rabbitmq_amqp1_0.tcp_listen_options</code>, and
          related config keys to reduce the amount of RAM by the
          server used per connection. This is often necessary in
          environments where the number of concurrent connections
          sustained per node is more important than throughput.
        </p>
        <p>
          The following example sets TCP buffers for AMQP 0-9-1 connections to 32 KiB:

<pre class="sourcecode ini">
tcp_listen_options.backlog = 128
tcp_listen_options.nodelay = true
tcp_listen_options.linger.on      = true
tcp_listen_options.linger.timeout = 0
tcp_listen_options.sndbuf  = 32768
tcp_listen_options.recbuf  = 32768
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
  ]}
].
</pre>

          The same example for MQTT and STOMP connections:

<pre class="sourcecode erlang">
[
  {rabbitmq_mqtt, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
                         ]},
  {rabbitmq_stomp, [
    {tcp_listen_options, [
                          {backlog,       128},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false},
                          {sndbuf,        32768},
                          {recbuf,        32768}
                         ]}
  ]}
].
</pre>

         Note that lowering TCP buffer sizes will result in a proportional throughput drop,
         so an optimal value between throughput and per-connection RAM use needs to be
         found for every workload.

         Setting send and receive buffer sizes to different values is dangerous
         and is not recommended. Values lower than 8 KiB are not recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-channel-max">
        <doc:heading>Limiting Number of Channels on a Connection</doc:heading>

        <p>
          Channels also consume RAM. By optimising how many channels applications use, that amount
          can be decreased. It is possible to cap the max number of channels on a connection using
          the <code>channel_max</code> configuration setting:

<pre class="sourcecode ini">
channel_max = 16
</pre>

          Note that some libraries and tools that build on top of RabbitMQ clients may implicitly require
          a certain number of channels. Finding an optimal value is usually a matter of trial and error.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-nodelay">
        <doc:heading>Nagle's Algorithm ("nodelay")</doc:heading>
        <p>
          Disabling <a
          href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's
          algorithm</a> is primarily useful for reducing latency but
          can also improve
          throughput. <code>kernel.inet_default_connect_options</code>
          and <code>kernel.inet_default_listen_options</code> must
          include <code>{nodelay, true}</code> to disable Nagle's
          algorithm for inter-node connections.  When configuring
          sockets that serve client connections,
          <code>rabbit.tcp_listen_options</code> must include the same
          option. This is the default.

          The following example demonstrates that:

In <code>rabbitmq.conf</code>

<pre class="sourcecode ini">
tcp_listen_options.backlog = 4096
tcp_listen_options.nodelay = true
</pre>

together with the following bits in the <a href="/configure.html#advanced-config-file">advanced config file</a>:

<pre class="sourcecode erlang">
[
  {kernel, [
    {inet_default_connect_options, [{nodelay, true}]},
    {inet_default_listen_options,  [{nodelay, true}]}
  ]}].
</pre>

When using the <a href="/configure.html#erlang-term-config-file">classic config format</a>,
everything is configured in a single file:

<pre class="sourcecode erlang">
[
  {kernel, [
    {inet_default_connect_options, [{nodelay, true}]},
    {inet_default_listen_options,  [{nodelay, true}]}
  ]},
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       4096},
                          {nodelay,       true},
                          {linger,        {true,0}},
                          {exit_on_close, false}
                         ]}
  ]}
].
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-async-thread-pool">
        <doc:heading>Erlang VM I/O Thread Pool Tuning</doc:heading>
        <p>
          Adequate Erlang VM I/O thread pool size is also important when tuning for a large number of
          concurrent connections. See the section above.
        </p>
      </doc:subsection>

      <doc:subsection name="tuning-for-large-number-of-connections-connection-backlog">
        <doc:heading>Connection Backlog</doc:heading>
        <p>
          With a low number of clients, new connection rate is very unevenly distributed
          but is also small enough to not make much difference. When the number reaches tens of thousands
          or more, it is important to make sure that the server can accept inbound connections.
          Unaccepted TCP connections are put into a queue with bounded length. This length has to be
          sufficient to account for peak load hours and possible spikes, for instance, when many clients
          disconnect due to a network interruption or choose to reconnect.
          This is configured using the <code>rabbit.tcp_listen_options.backlog</code>
          option:

<pre class="sourcecode ini">
tcp_listen_options.backlog = 4096
tcp_listen_options.nodelay = true
</pre>

In the <a href="/configure.html#erlang-term-config-file">classic config format</a>:

<pre class="sourcecode erlang">
[
  {rabbit, [
    {tcp_listen_options, [
                          {backlog,       4096},
                          {nodelay,       true},
                          {linger,        {true, 0}},
                          {exit_on_close, false}
                         ]}
  ]}
].
</pre>

           Default value is 128. When pending connection queue length grows beyond this value,
           connections will be rejected by the operating system. See also <code>net.core.somaxconn</code>
           in the kernel tuning section.
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="os-tuning">
      <doc:heading>OS Level Tuning</doc:heading>

      <doc:subsection name="os-tuning-intro">
        <p>
          Operating system settings can affect operation of RabbitMQ.
          Some are directly related to networking (e.g. TCP settings), others
          affect TCP sockets as well as other things (e.g. open file handles limit).

          Understanding these limits is important, as they may change depending on
          the workload.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-important-options">
        <p>
          A few important configurable kernel options include (for IPv4):

          <dl>
            <dt><code>fs.file-max</code></dt>
            <dd>
              Max number of files the kernel will allocate. Limits and current value
              can be inspected using <code>/proc/sys/fs/file-nr</code>.
            </dd>

            <dt><code>net.ipv4.ip_local_port_range</code></dt>
            <dd>
              Local IP port range, define as a pair of values. The range must provide enough
              entries for the peak number of concurrent connections.
            </dd>

            <dt><code>net.ipv4.tcp_tw_reuse</code></dt>
            <dd>
              When enabled, allows the kernel to reuse sockets in <code>TIME_WAIT</code>
              state when it's safe to do so. See <a href="http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html">Coping with the TCP TIME_WAIT connections on busy servers</a> for details.
              This option is dangerous when used behind NAT.
            </dd>

            <dt><code>net.ipv4.tcp_fin_timeout</code></dt>
            <dd>
              Lowering this value to 5-10 reduces the amount of time closed connections
              will stay in the TIME_WAIT state. Recommended for cases when a large
              number of concurrent connections is expected.
            </dd>

            <dt><code>net.core.somaxconn</code></dt>
            <dd>
              Size of the listen queue (how many connections are in
              the process of being established at the same time).
              Default is 128. Increase to 4096 or higher to support
              inbound connection bursts, e.g. when clients reconnect
              en masse.
            </dd>

            <dt><code>net.ipv4.tcp_max_syn_backlog</code></dt>
            <dd>
              Maximum number of remembered connection requests which
              did not receive an acknowledgment yet from
              connecting client. Default is 128, max value is 65535. 4096 and 8192 are
              recommended starting values when optimising for throughput.
            </dd>

            <dt><code>net.ipv4.tcp_keepalive_*</code></dt>
            <dd>
              <code>net.ipv4.tcp_keepalive_time</code>, <code>net.ipv4.tcp_keepalive_intvl</code>,
              and <code>net.ipv4.tcp_keepalive_probes</code> configure TCP keepalive.

              AMQP 0-9-1 and STOMP have <a href="/heartbeats.html">Heartbeats</a> which partially
              undo its effect, namely that it can take minutes to detect an unresponsive peer,
              e.g. in case of a hardware or power failure. MQTT also has its own keepalives
              mechanism which is the same idea under a different name.

              When enabling TCP keepalive with default settings, we
              recommend setting heartbeat timeout to 8-20 seconds. Also see a note on TCP keepalives
              later in this guide.
            </dd>

            <dt><code>net.ipv4.conf.default.rp_filter</code></dt>
            <dd>
              Enabled reverse path filtering. If <a href="http://en.wikipedia.org/wiki/IP_address_spoofing">IP address spoofing</a>
              is not a concern for your system, disable it.
            </dd>
          </dl>

          Note that default values for these vary between Linux kernel releases and distributions.
          Using a recent kernel (3.9 or later) is recommended.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-important-how-to-configure">
        <p>
          Kernel parameter tuning differs from OS to OS. This guide focuses on Linux.
          To configure a kernel parameter interactively, use <code>sysctl -w</code> (requires superuser
          privileges), for example:

<pre class="sourcecode bash">
sysctl -w fs.file-max 200000
</pre>

          To make the changes permanent (stick between reboots), they need to be added to
          <code>/etc/sysctl.conf</code>. See <a href="http://man7.org/linux/man-pages/man8/sysctl.8.html">sysctl(8)</a>
          and <a href="http://man7.org/linux/man-pages/man5/sysctl.conf.5.html">sysctl.conf(5)</a>
          for more details.
        </p>
      </doc:subsection>

      <doc:subsection name="os-tuning-outro">
        <p>
          TCP stack tuning is a broad topic that is covered in much detail elsewhere:

          <ul>
            <li><a href="https://psc.edu/index.php/services/networking/68-research/networking/641-tcp-tune">Enabling High Performance Data Transfers</a></li>
            <li><a href="https://fasterdata.es.net/network-tuning/">Network Tuning Guide</a></li>
          </ul>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="socket-gen-tcp-options">
      <doc:heading>TCP Socket Options</doc:heading>

      <doc:subsection name="socket-gen-tcp-options-common">
        <doc:heading>Common Options</doc:heading>
        <dl>
          <dt><code>tcp_listen_options.nodelay</code></dt>
          <dd>
            When set to <code>true</code>, disables
            <a href="http://en.wikipedia.org/wiki/Nagle's_algorithm">Nagle's algorithm</a>.
            Default is true. Highly recommended for most users.
          </dd>
          <dt><code>tcp_listen_options.sndbuf</code></dt>
          <dd>
            See TCP buffers discussion earlier in this guide. Default value is
            automatically tuned by the OS, typically in the 88 KiB to 128 KiB range on
            modern Linux versions. Increasing buffer size improves consumer throughput
            and RAM use for every connection. Decreasing has the opposite effect.
          </dd>
          <dt><code>tcp_listen_options.recbuf</code></dt>
          <dd>
            See TCP buffers discussion earlier in this guide. Default value effects
            are similar to that of <code>rabbit.tcp_listen_options.sndbuf</code> but
            for publishers and protocol operations in general.
          </dd>
          <dt><code>tcp_listen_options.backlog</code></dt>
          <dd>
            Maximum size of the unaccepted TCP connections queue. When this size
            is reached, new connections will be rejected. Set to 4096 or higher for
            environments with thousands of concurrent connections and possible bulk client
            reconnections.
          </dd>
          <dt><code>tcp_listen_options.keepalive</code></dt>
          <dd>
            When set to <code>true</code>, enables TCP keepalives (see above).
            Default is <code>false</code>. Makes sense for environments where
            connections can go idle for a long time (at least 10 minutes),
            although using <a href="/heartbeats.html">heartbeats</a> is still recommended over
            this option.
          </dd>
        </dl>

      </doc:subsection>

      <doc:subsection name="socket-gen-tcp-options-defaults">
        <doc:heading>Defaults</doc:heading>

        <p>
          Below is the default TCP socket option configuration used by RabbitMQ:

          <ul>
            <li>TCP connection backlog is limited to 128 connections</li>
            <li>Nagle's algorithm is disabled</li>
            <li>Server socket lingering is enabled with the timeout of 0</li>
          </ul>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="heartbeats">
      <doc:heading>Heartbeats</doc:heading>

      <p>
        Some protocols supported by RabbitMQ, including AMQP 0-9-1, support <em>heartbeats</em>, a way to detect dead
        TCP peers quicker. Please refer to the <a href="/heartbeats.html">Heartbeats guide</a>
        for more information.
      </p>
    </doc:section>

    <doc:section name="nettick">
      <doc:heading>Net Tick Time</doc:heading>

      <p>
        <a href="/heartbeats.html">Heartbeats</a> are used to detect peer or connection failure
        between clients and RabbitMQ nodes. <a href="/nettick.html">net_ticktime</a> serves
        the same purpose but for cluster node communication. Values lower than 5 (seconds)
        may result in false positive and are not recommended.
      </p>
    </doc:section>

    <doc:section name="tcp-keepalives">
      <doc:heading>TCP Keepalives</doc:heading>
      <p>
        TCP contains a mechanism similar in purpose to the heartbeat
        (a.k.a. keepalive) one in messaging protocols and net tick
        timeout covered above: TCP keepalives. Due to inadequate
        defaults, TCP keepalives often don't work the way they are
        supposed to: it takes a very long time (say, an hour or more)
        to detect a dead peer. However, with tuning they can serve
        the same purpose as heartbeats and clean up stale TCP connections
        e.g. with clients that opted to not use heartbeats, intentionally or
        not.

        Below is an example sysctl configuration for TCP keepalives
        that considers TCP connections dead or unreachable after 120
        seconds (4 attempts every 15 seconds after connection idle for 60 seconds):

<pre class="sourcecode sysctl">
net.ipv4.tcp_keepalive_time=60
net.ipv4.tcp_keepalive_intvl=15
net.ipv4.tcp_keepalive_probes=4
</pre>

        TCP keepalives can be a useful additional defense mechanism
        in environments where RabbitMQ operator has no control
        over application settings or client libraries used.
      </p>
    </doc:section>

    <doc:section name="handshake-timeout">
      <doc:heading>Connection Handshake Timeout</doc:heading>

      <p>
        RabbitMQ has a timeout for connection handshake, 10 seconds by
        default. When clients run in heavily constrained environments,
        it may be necessary to increase the timeout. This can be done via
        the <code>rabbit.handshake_timeout</code> (in milliseconds):
<pre class="code">
    handshake_timeout = 20000
</pre>

Using the classic config format:

<pre class="sourcecode erlang">
[
  {rabbit, [
    %% 20 seconds
    {handshake_timeout, 20000}
  ]}
].
</pre>

        It should be pointed out that this is only necessary with very constrained
        clients and networks. Handshake timeouts in other circumstances indicate
        a problem elsewhere.
      </p>

      <doc:subsection name="tls-handshake">
        <doc:heading>TLS/SSL Handshake</doc:heading>

        <p>
          If TLS/SSL is enabled, it may necessary to increase also the TLS/SSL
          handshake timeout. This can be done via
          the <code>rabbit.ssl_handshake_timeout</code> (in milliseconds):

<pre class="sourcecode ini">
ssl_handshake_timeout = 10000
</pre>

Using the classic config format:

<pre class="sourcecode erlang">
[
  {rabbit, [
    %% 10 seconds
    {ssl_handshake_timeout, 10000}
  ]}
].
</pre>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="dns">
      <doc:heading>Hostname Resolution and DNS</doc:heading>

      <p>
        In many cases, RabbitMQ relies on the Erlang runtime for inter-node communication (including
        tools such as <code>rabbitmqctl</code>, <code>rabbitmq-plugins</code>, etc). Client libraries
        also perform hostname resolution when connecting to RabbitMQ nodes. This section briefly
        covers most common issues associated with that.
      </p>


      <doc:subsection name="dns-resolution-by-clients">
        <doc:heading>Performed by Client Libraries</doc:heading>

        <p>
          If a client library is configured to connect to a hostname, it performs
          hostname resolution. Depending on DNS and local resolver (<code>/etc/hosts</code>
          and similar) configuration, this can take some time. Incorrect configuration
          may lead to resolution timeouts, e.g. when trying to resolve a local hostname
          such as <code>my-dev-machine</code>, over DNS. As a result, client connections
          can take a long time (from tens of seconds to a few minutes).
        </p>
      </doc:subsection>

      <doc:subsection name="dns-resolution-by-nodes">
        <doc:heading>Short and Fully-qualified RabbitMQ Node Names</doc:heading>

        <p>
          RabbitMQ relies on the Erlang runtime for inter-node
          communication. Erlang nodes include a hostname, either short
          (<code>rmq1</code>) or fully-qualified
          (<code>rmq1.dev.megacorp.local</code>). Mixing short and
          fully-qualified hostnames is not allowed by the
          runtime. Every node in a cluster must be able to resolve
          every other node's hostname, short or fully-qualified.

          By default RabbitMQ will use short hostnames. Set the
          <code>RABBITMQ_USE_LONGNAME</code> environment variable to
          make RabbitMQ nodes use fully-qualified names,
          e.g. <code>rmq1.dev.megacorp.local</code>.
        </p>
      </doc:subsection>

      <doc:subsection name="dns-reverse-dns-lookups">
        <doc:heading>Reverse DNS Lookups</doc:heading>

        <p>
          If the <code>rabbit.reverse_dns_lookups</code> configuration option is set to <code>true</code>,
          RabbitMQ will perform reverse DNS lookups for client IP addresses and list hostnames
          in connection information (e.g. in the <a href="/management.html">Management UI</a>).
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="logging">
      <doc:heading>Connection Event Logging</doc:heading>

      <p>
        See <a href="/logging.html#connection-lifecycle-events">Connection Lifecycle Events</a> in the logging guide.
      </p>
    </doc:section>

    <doc:section name="troubleshooting-where-to-start">
      <doc:heading>Troubleshooting Network Connectivity</doc:heading>

      <p>
        <a href="/troubleshooting-networking.html">Troubleshooting of networking-related issues</a> is covered in a separate guide.
      </p>
    </doc:section>
  </body>
</html>
