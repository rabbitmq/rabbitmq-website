<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!--
Copyright (c) 2007-2018 Pivotal Software, Inc.

All rights reserved. This program and the accompanying materials
are made available under the terms of the under the Apache License,
Version 2.0 (the "License”); you may not use this file except in compliance
with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc"
    xmlns:x="http://www.rabbitmq.com/2011/extensions"
    xmlns:xi="http://www.w3.org/2003/XInclude">
  <head>
    <title>Cluster Formation and Peer Discovery</title>
  </head>
  <body>
    <doc:section name="intro">
      <doc:heading>Introduction</doc:heading>

      <p>
        This guide covers various automation-oriented cluster formation and
        peer discovery features. For a general overview of RabbitMQ clustering,
        please refer to the <a href="/clustering.html">Clustering Guide</a>.
      </p>

      <p>
        This guide assumes general familiarity with <a href="/clustering.html">RabbitMQ clustering</a>
        and focuses on the peer discovery subsystem.
        For example, it will not cover what <a href="/networking.html">ports must be
        open</a> for inter-node communication, how nodes authenticate to each other, and so on.
        Besides discovery mechanisms, this guide also covers the problem of <a href="#initial-formation-race-condition">initial
        cluster formation</a> with nodes booting in parallel as well as <a href="#node-health-checks-and-cleanup">additional health checks</a> offered
        by some discovery implementations.
      </p>

    </doc:section>


    <doc:section name="peer-discovery">
      <doc:heading>How Peer Discovery Works</doc:heading>

      <p>
        To form a cluster, new ("blank") nodes need to be able to discover
        their peers. This can be done using a variety of mechanisms (backends).
        Some mechanisms assume all cluster members are known ahead of time (for example, listed
        in the config file), others are dynamic (nodes can come and go).
      </p>

      <p>
        All peer discovery mechanisms assume that newly joining nodes will be able to
        contact their peers in the cluster and authenticate with them successfully.
        The mechanisms that rely on an external service (e.g. DNS or Consul) or API (e.g. AWS or Kubernetes)
        require the service(s) to be available and reachable over HTTP(S) on their standard ports.
        Inability to reach the service will lead to node's inability to join the cluster.
      </p>

      <p>
        The discovery mechanism to use is specified in the <a
          href="/configure.html">config file</a>, as are mechanism-specific
        settings, for example, discovery service hostnames, credentials, and so
        on. <code>cluster_formation.peer_discovery_backend</code> is the key
        that controls what discovery module (implementation) is used:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_classic_config
</pre>

        The module has to implement the <a
          href="https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_peer_discovery_backend.erl">rabbit_peer_discovery_backend</a>
        behaviour. Plugins therefore can introduce their own discovery
        mechanisms.
      </p>

      <p>
        The following mechanisms are built into the core:

        <ul class="plain">
          <li><a href="#peer-discovery-classic-config">Config file</a></li>
          <li><a href="#peer-discovery-dns">Pre-configured DNS A/AAAA records</a></li>
        </ul>

        Additional peer discovery mechanisms are available via plugins. The following
        peer discovery plugins ship with RabbitMQ as of 3.7.0:

        <ul class="plain">
          <li><a href="#peer-discovery-aws">AWS (EC2)</a></li>
          <li><a href="#peer-discovery-k8s">Kubernetes</a></li>
          <li><a href="#peer-discovery-consul">Consul</a></li>
          <li><a href="#peer-discovery-etcd">etcd</a></li>
        </ul>
      </p>

      <p>
        The above plugins do not need to be installed but they need
        to be enabled before node start using <a href="">rabbitmq-plugins</a>' <code>--offline</code> mode:

        <pre class="sourcecode ini">
rabbitmq-plugins --offline enable [plugin name]
</pre>
      </p>

      <p>
        When a node starts and detects it doesn't have a previously
        initialised database, it will check if there's a peer
        discovery mechanism configured. If that's the case, it will
        then perform the discovery and attempt to contact each
        discovered peer in order. Finally, it will attempt to join the
        cluster of the first reachable peer.
      </p>

      <p>
        Depending on the backend (mechanism) used, the process of peer discovery may involve
        contacting external services, for example, an AWS API endpoint, a Consul node or
        performing a DNS query. Some backends require nodes to register (tell the backend that the
        node is up and should be counted as a cluster member): for example, Consul and etcd both
        support registration. With other backends the list of nodes is configured ahead of
        time (e.g. config file). Those backends are said to not support node registration.

        In some cases node registration is implicit or managed by an external service.
        AWS autoscaling groups is a good example: AWS keeps track of group membership,
        so nodes don't have to (or cannot) explicitly register. However, the list of cluster members
        is not predefined. Such backends usually include a no-op registration step
        and apply one of the <a href="#initial-formation-race-condition">race condition mitigation mechanisms</a> described below.
      </p>

      <p>
        When a cluster is first formed and there are no registered nodes yet,
        a natural race condition between booting
        nodes occurs. Different backends <a href="#initial-formation-race-condition">address this problem</a>
        differently: some try to acquire a lock with an external service, others rely on randomized
        delays. This problem does not apply to the backends that require listing all nodes ahead of time.
      </p>

      <p>
        When the configured backend supports registration, nodes unregister when they stop.
      </p>

      <p>
        If peer discovery isn't configured, or it fails, or no peers are reachable,
        a node that wasn't a cluster member in the past
        will initialise from scratch and proceed as a standalone node.
      </p>

      <p>
        If a node previously was a cluster member, it will try to contact
        its "last seen" peer for a period of time. In this case, no peer discovery
        will be performed. This is true for all backends.
      </p>
    </doc:section>


    <doc:section name="rejoining">
      <doc:heading>Peer Rejoining Timeout</doc:heading>

      <p>
        If a node previously was a cluster member, when it boots it will try to contact
        its "last seen" peer for a period of time. If the peer is not booted (e.g. when
        a full cluster restart or upgrade is performed) or cannot be reached, the node will
        retry the operation a number of times.

        Default values are <code>10</code> retries and <code>30</code> seconds per attempt,
        respectively, or 5 minutes total. In environments where nodes can take a long and/or uneven
        time to start it is recommended that the number of retries is increased.
      </p>
    </doc:section>

    <doc:section name="peer-discovery-classic-config">
      <doc:heading>Config File Peer Discovery Backend</doc:heading>

      <p>
        The most basic way for a node to discover its cluster peers is to read a list
        of nodes from the config file.
      </p>
      <p>
        This is done using the <code>cluster_formation.classic_config.nodes</code> config setting.

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_classic_config

cluster_formation.classic_config.nodes.1 = rabbit@hostname1.eng.example.local
cluster_formation.classic_config.nodes.2 = rabbit@hostname2.eng.example.local
</pre>
      </p>
      <p>
        The following example demonstrates the same configuration in
        the <a href="/configure.html#config-file-formats">classic config format</a>. The 2nd member of the
        <code>rabbit.cluster_nodes</code> tuple is the node type to
        use for the current node. In the vast majority of cases all
        nodes should be <code>disc</code> nodes.

        <pre class="sourcecode erlang">
[
 {rabbit, [
           {cluster_nodes, {['rabbit@hostname1.eng.example.local',
                             'rabbit@hostname2.eng.example.local'], disc}}
          ]}
].
</pre>
      </p>
    </doc:section>

    <doc:section name="peer-discovery-dns">
      <doc:heading>DNS Peer Discovery Backend</doc:heading>

      <p>
        Another built-in peer discovery mechanism as of RabbitMQ 3.7.0 is DNS-based.
        It relies on a pre-configured hostname ("seed hostname") with DNS A (or AAAA) records and reverse DNS lookups
        to perform peer discovery. More specifically, this mechanism will perform the following steps:

        <ul>
          <li>Query DNS A records of the seed hostname.</li>
          <li>For each returned DNS record's IP address, perform a reverse DNS lookup.</li>
          <li>
            Append current node's prefix (e.g. <code>rabbit</code> in <code>rabbit@hostname1.example.local</code>)
            to each hostname and return the result.
          </li>
        </ul>
      </p>

      <p>
        For example, let's consider a seed hostname of
        <code>discovery.eng.example.local</code>.  It has 2 DNS A
        records that return two IP addresses:
        <code>192.168.100.1</code> and
        <code>192.168.100.2</code>. Reverse DNS lookups for those IP
        addresses return <code>node1.eng.example.local</code> and
        <code>node2.eng.example.local</code>, respectively. Current node's name is not
        set and defaults to <code>rabbit@$(hostname)</code>.
        The final list of nodes discovered will contain two nodes: <code>rabbit@node1.eng.example.local</code>
        and <code>rabbit@node2.eng.example.local</code>.
      </p>

      <p>
        The seed hostname is set using the <code>cluster_formation.dns.hostname</code> config setting:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_dns

cluster_formation.dns.hostname = discovery.eng.example.local
</pre>
      </p>
    </doc:section>

    <doc:section name="peer-discovery-aws">
      <doc:heading>Peer Discovery on AWS (EC2)</doc:heading>

      <p>
        An <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-aws">AWS (EC2)-specific</a> discovery mechanism
        is available via a plugin. It provides two ways for a node to discover its peers:

        <ul>
          <li>Using EC2 instance tags</li>
          <li>Using AWS autoscaling group membership</li>
        </ul>

        Both methods rely on AWS-specific APIs (endpoints) and features and thus cannot work in
        other IaaS environments. Once a list of cluster member instances is retrieved,
        final node names are computed using instance hostnames or IP addresses.
      </p>

      <p>
        When the AWS peer discovery mechanism is used, nodes will
        delay their startup for a randomly picked value to reduce the
        probability of a <a href="#initial-formation-race-condition">race condition during initial cluster
        formation</a>.
      </p>

      <doc:subsection name="peer-discovery-aws-credentials">
        <doc:heading>Configuration and Credentials</doc:heading>

        <p>
          Before a node can perform any operations on AWS, it needs to have a set of
          AWS account credentials configured. This can be done in a couple of ways:

          <ol>
            <li>Via <a href="/configure.html">config file</a></li>
            <li>Using environment variables</li>
          </ol>

          <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">EC2 Instance Metadata service</a> for the region will also be consulted.
        </p>

        <p>
          The following example snippet configures RabbitMQ to use the AWS peer discovery
          backend and provides information about AWS region as well as a set of credentials:

          <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_aws

cluster_formation.aws.region = us-east-1
cluster_formation.aws.access_key_id = ANIDEXAMPLE
cluster_formation.aws.secret_key = WjalrxuTnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY
</pre>

          If region is left unconfigured, <code>us-east-1</code> will be used by default.
          Sensitive values in configuration file can optionally <a href="/configure.html#configuration-encryption">be encrypted</a>.
        </p>

        <p>
          If an <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html">IAM role is assigned to EC2 instances</a> running RabbitMQ nodes,
          a policy has to be used to allow said instances use EC2 Instance Metadata Service.
          Below is an example of such policy:

          <pre class="sourcecode json">
{
"Version": "2012-10-17",
"Statement": [
              {
              "Effect": "Allow",
              "Action": [
                         "autoscaling:DescribeAutoScalingInstances",
                         "ec2:DescribeInstances"
                         ],
              "Resource": [
                           "*"
                           ]
              }
              ]
}
</pre>
        </p>
      </doc:subsection>

      <doc:subsection name="peer-discovery-aws-autoscaling-group-membership">
        <doc:heading>Using Autoscaling Group Membership</doc:heading>

        <p>
          When autoscaling-based peer discovery is used, current node's EC2 instance autoscaling
          group members will be listed and used to produce the list of discovered peers.
        </p>

        <p>
          To use autoscaling group membership, set the <code>cluster_formation.aws.use_autoscaling_group</code> key
          to <code>true</code>:

          <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_aws

cluster_formation.aws.region = us-east-1
cluster_formation.aws.access_key_id = ANIDEXAMPLE
cluster_formation.aws.secret_key = WjalrxuTnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY

cluster_formation.aws.use_autoscaling_group = true
</pre>


        </p>
      </doc:subsection>

      <doc:subsection name="peer-discovery-aws-tags">
        <doc:heading>Using EC2 Instance Tags</doc:heading>

        <p>
          When tags-based peer discovery is used, the plugin will list EC2 instances
          using EC2 API and filter them by configured instance tags. Resulting instance set
          will be used to produce the list of discovered peers.
        </p>

        <p>
          Tags are configured using the <code>cluster_formation.aws.instance_tags</code> key. The example
          below uses three tags: <code>region</code>, <code>service</code>, and <code>environment</code>.

          <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_aws

cluster_formation.aws.region = us-east-1
cluster_formation.aws.access_key_id = ANIDEXAMPLE
cluster_formation.aws.secret_key = WjalrxuTnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY

cluster_formation.aws.instance_tags.region = us-east-1
cluster_formation.aws.instance_tags.service = rabbitmq
cluster_formation.aws.instance_tags.environment = staging
</pre>


        </p>
      </doc:subsection>

      <doc:subsection name="peer-discovery-aws-other-settings">
        <doc:heading>Using Private EC2 Instance IPs</doc:heading>

        <p>
          By default peer discovery will use private DNS hostnames to compute node names.
          It is possible to opt into using private IPs instead by setting
          the <code>cluster_formation.aws.aws_use_private_ip</code> key to <code>true</code>:

          <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_aws

cluster_formation.aws.region = us-east-1
cluster_formation.aws.access_key_id = ANIDEXAMPLE
cluster_formation.aws.secret_key = WjalrxuTnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY

cluster_formation.aws.use_autoscaling_group = true
cluster_formation.aws.use_private_ip = true
</pre>
        </p>
      </doc:subsection>
    </doc:section>

    <doc:section name="peer-discovery-k8s">
      <doc:heading>Peer Discovery on Kubernetes</doc:heading>

      <p>
        An <a href="https://kubernetes.io/">Kubernetes</a>-based discovery mechanism
        is available via <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s">a plugin</a>.
      </p>

      <p>
        With this mechanism, nodes fetch a list of their peers from
        the Kubernetes API endpoint using a set of configured values:
        a URI scheme, host, port, as as well as the token and
        certificate paths.
      </p>

      <p>
        It is highly recommended that RabbitMQ clusters are deployed using a <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#statefulset">stateful set</a>. If a stateless set is used
        recreated nodes will not have their persisted data and will start as blank nodes.
        This can lead to data loss and higher network traffic volume due to more frequent
        <a href="ha.html">eager synchronisation</a> of newly joining nodes. Stateless sets are also
        prone to the <a href="#initial-formation-race-condition">natural race condition</a> during initial
        cluster formation, unlike stateful sets that initialise pods <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#understanding-stateful-pod-initialization">one by one</a>.
      </p>

      <p>
        To use Kubernetes for peer discovery, set the <code>cluster_formation.peer_discovery_backend</code>
        to <code>rabbit_peer_discovery_k8s</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

# Kubernetes API hostname (or IP address). Default value is kubernetes.default.svc.cluster.local
cluster_formation.k8s.host = kubernetes.default.example.local
</pre>
      </p>

      <p>
        It is possible to configure Kubernetes API port and URI scheme:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local
# 443 is used by default
cluster_formation.k8s.port = 443
# https is used by default
cluster_formation.k8s.scheme = https
</pre>
      </p>

      <p>
        Kubernetes token file path is configurable via <code>cluster_formation.k8s.token_path</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local
# default value is /var/run/secrets/kubernetes.io/serviceaccount/token
cluster_formation.k8s.token_path = /var/run/secrets/kubernetes.io/serviceaccount/token
</pre>

        It must point to a local file that exists and is readable by RabbitMQ.
      </p>

      <p>
        Certificate and namespace paths use <code>cluster_formation.k8s.cert_path</code>
        and <code>cluster_formation.k8s.namespace_path</code>, respectively:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local
# default value is /var/run/secrets/kubernetes.io/serviceaccount/token
cluster_formation.k8s.token_path = /var/run/secrets/kubernetes.io/serviceaccount/token

# default value is /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
cluster_formation.k8s.cert_path = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

# default value is /var/run/secrets/kubernetes.io/serviceaccount/namespace
cluster_formation.k8s.namespace_path = /var/run/secrets/kubernetes.io/serviceaccount/namespace
</pre>

        Just like with the token path key both must point to a local
        file that exists and is readable by RabbitMQ.
      </p>

      <p>
        When a list of peer nodes is computed from a list of pod containers returned by Kubernetes,
        either contain hostnames or IP addresses can be used. This is configurable using the
        <code>cluster_formation.k8s.address_type</code> key:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local

cluster_formation.k8s.token_path = /var/run/secrets/kubernetes.io/serviceaccount/token
cluster_formation.k8s.cert_path = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
cluster_formation.k8s.namespace_path = /var/run/secrets/kubernetes.io/serviceaccount/namespace

# should result set use hostnames or IP addresses
# of Kubernetes API-reported containers?
# supported values are "hostname" and "ip"
cluster_formation.k8s.address_type = ip
</pre>

        Supported values are <code>ip</code> or <code>hostname</code>, the former
        is used by default.
      </p>

      <p>
        It is possible to append a suffix to peer hostnames returned by Kubernetes using
        <code>cluster_formation.k8s.hostname_suffix</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local

cluster_formation.k8s.token_path = /var/run/secrets/kubernetes.io/serviceaccount/token
cluster_formation.k8s.cert_path = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
cluster_formation.k8s.namespace_path = /var/run/secrets/kubernetes.io/serviceaccount/namespace

# no suffix is appended by default
cluster_formation.k8s.hostname_suffix = rmq.eng.example.local
</pre>
      </p>

      <p>
        Service name is <code>rabbitmq</code> by default but can be overridden using the
        <code>cluster_formation.k8s.service_name</code> key if needed:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local

cluster_formation.k8s.token_path = /var/run/secrets/kubernetes.io/serviceaccount/token
cluster_formation.k8s.cert_path = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
cluster_formation.k8s.namespace_path = /var/run/secrets/kubernetes.io/serviceaccount/namespace

# overrides Kubernetes service name. Default value is "rabbitmq".
cluster_formation.k8s.service_name = rmq-qa
</pre>
      </p>


      <p>
        As mentioned above, stateful sets is the recommended way of running RabbitMQ on Kubernetes.
        Stateful set pods are initialised one at a time. That effectively addresses
        the natural <a href="#initial-formation-race-condition">race condition during the initial cluster formation</a>.
        Randomized startup delay in such scenarios can use a significantly lower delay value range (e.g. 0 to 1 second):

        <pre class="sourcecode ini">
cluster_formation.randomized_startup_delay_range.min = 0
cluster_formation.randomized_startup_delay_range.max = 2

cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s

cluster_formation.k8s.host = kubernetes.default.example.local

# ...
</pre>
      </p>
    </doc:section>

    <doc:section name="peer-discovery-consul">
      <doc:heading>Peer Discovery Using Consul</doc:heading>

      <p>
        A <a href="https://www.consul.io">Consul</a>-based discovery mechanism
        is available via <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-consul">a plugin</a>.
        Consul 0.8.0 and later versions are supported.
      </p>

      <p>
        Nodes register with Consul on boot and unregister when they
        leave. Prior to registration, nodes will attempt to acquire a
        lock in Consul to reduce the probability of a <a href="#initial-formation-race-condition">race condition
        during initial cluster formation</a>.
        When a node registers with Consul, it will set up a periodic <a href="https://www.consul.io/docs/agent/checks.html">health
        check</a> for itself (more on this below).
      </p>

      <p>
        To use Consul for peer discovery, set the <code>cluster_formation.peer_discovery_backend</code> to
        to <code>rabbit_peer_discovery_consul</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

# Consul host (hostname or IP address). Default value is localhost
cluster_formation.consul.host = consul.eng.example.local
</pre>
      </p>
      <p>
        It is possible to configure Consul port and URI scheme:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# 8500 is used by default
cluster_formation.consul.port = 8500
# http is used by default
cluster_formation.consul.scheme = http
</pre>
      </p>

      <p>
        To configure <a href="https://www.consul.io/docs/guides/acl.html">Consul ACL</a> token,
        use :

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
cluster_formation.consul.acl_token = acl-token-value
</pre>
      </p>

      <p>
        Service name (as registered in Consul) defaults to "rabbitmq" but can be overridden:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# rabbitmq is used by default
cluster_formation.consul.svc = rabbitmq
</pre>
      </p>

      <p>
        Service hostname (address) as registered in Consul will be fetched by peers
        and therefore must resolve on all nodes.
        The hostname can be computed by the plugin or specified by the user. When computed automatically,
        a number of nodes and OS properties can be used:

        <ul>
          <li>Hostname (as returned by <code>gethostname(2)</code>)</li>
          <li>Node name (without the <code>rabbit@</code> prefix)</li>
          <li>IP address of an NIC (network controller interface)</li>
        </ul>

        When <code>cluster_formation.consul.svc_addr_auto</code> is set to <code>false</code>,
        service name will be taken as is from <code>cluster_formation.consul.svc_addr</code>.
         When it is set to <code>true</code>,
        other options explained below come into play.
      </p>

      <p>
        In the following example, the service address reported to Consul is
        hardcoded to <code>hostname1.rmq.eng.example.local</code> instead of being computed automatically
        from the environment:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local

cluster_formation.consul.svc = rabbitmq
# do not compute service address, it will be specified below
cluster_formation.consul.svc_addr_auto = false
# service address, will be communicated to other nodes
cluster_formation.consul.svc_addr = hostname1.rmq.eng.example.local
# use long RabbitMQ node names?
cluster_formation.consul.use_longname = true
</pre>
      </p>

      <p>
        In this example, the service address reported to Consul is
        parsed from node name (the <code>rabbit@</code> prefix will be dropped):

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local

cluster_formation.consul.svc = rabbitmq
# do compute service address
cluster_formation.consul.svc_addr_auto = true
# compute service address using node name
cluster_formation.consul.svc_addr_use_nodename = true
# use long RabbitMQ node names?
cluster_formation.consul.use_longname = true
</pre>

        <code>cluster_formation.consul.svc_addr_use_nodename</code> is a boolean
        field that instructs Consul peer discovery backend to compute service address
        using RabbitMQ node name.
      </p>

      <p>
        In the next example, the service address is
        computed using hostname as reported by the OS instead of node name:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local

cluster_formation.consul.svc = rabbitmq
# do compute service address
cluster_formation.consul.svc_addr_auto = true
# compute service address using host name and not node name
cluster_formation.consul.svc_addr_use_nodename = false
# use long RabbitMQ node names?
cluster_formation.consul.use_longname = true
</pre>
      </p>

      <p>
        In the example below, the service address is
        computed by taking the IP address of a provided NIC, <code>en0</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local

cluster_formation.consul.svc = rabbitmq
# do compute service address
cluster_formation.consul.svc_addr_auto = true
# compute service address using the IP address of a NIC, en0
cluster_formation.consul.svc_addr_nic = en0
cluster_formation.consul.svc_addr_use_nodename = false
# use long RabbitMQ node names?
cluster_formation.consul.use_longname = true
</pre>
      </p>

      <p>
        Service port as registered in Consul can be overridden. This is only
        necessary if RabbitMQ uses a <a href="/networking.html">non-standard port</a>
        for client (technically AMQP 0-9-1 and AMQP 1.0) connections since default value is 5672.

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# 5672 is used by default
cluster_formation.consul.svc_port = 6674
</pre>
      </p>

      <p>
        When a node registers with Consul, it will set up a periodic <a href="https://www.consul.io/docs/agent/checks.html">health
        check</a> for itself. Online nodes will periodically send a health check update to Consul to indicate the service
        is available. This interval can be configured:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# health check interval (node TTL) in seconds
# default: 30
cluster_formation.consul.svc_ttl = 40
</pre>

        A node that failed its <a href="https://www.consul.io/docs/agent/checks.html">health
        check</a> is considered to be in the warning state by Consul.
        Such nodes can be automatically unregistered by Consul after a
        period of time (note: this is a separate interval value from
        the TTL above). The period cannot be less than 60 seconds.

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# health check interval (node TTL) in seconds
cluster_formation.consul.svc_ttl = 30
# how soon should nodes that fail their health checks be unregistered by Consul?
# this value is in seconds and must not be lower than 60 (a Consul requirement)
cluster_formation.consul.deregister_after = 90
</pre>

        Please see a section on <a href="#node-health-checks-and-cleanup">automatic cleanup of nodes</a> below.
      </p>

      <p>
        Nodes in the warning state are excluded from peer discovery results
        by default. It is possible to opt into including them by setting
        <code>cluster_formation.consul.include_nodes_with_warnings</code> to
        <code>true</code>:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local
# health check interval (node TTL) in seconds
cluster_formation.consul.svc_ttl = 30
# include node in the warning state into discovery result set
cluster_formation.consul.include_nodes_with_warnings = true
</pre>
      </p>

      <p>
        If node name is computed and long node names are used, it is possible to
        append a suffix to node names retrieved from Consul. The format is
        <em>.node.{domain_suffix}</em>. This can be useful in environments with
        DNS conventions, e.g. when all service nodes
        are organised in a separate subdomain. Here's an example:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul

cluster_formation.consul.host = consul.eng.example.local

cluster_formation.consul.svc = rabbitmq
# do compute service address
cluster_formation.consul.svc_addr_auto = true
# compute service address using node name
cluster_formation.consul.svc_addr_use_nodename = true
# use long RabbitMQ node names?
cluster_formation.consul.use_longname = true
# append a suffix (node.rabbitmq.example.local) to node names retrieved from Consul
cluster_formation.consul.domain_suffix = example.local
</pre>

        With this setup node names will be computed to <code>rabbit@192.168.100.1.node.example.local</code>
        instead of <code>rabbit@192.168.100.1</code>.
      </p>
    </doc:section>

    <doc:section name="peer-discovery-etcd">
      <doc:heading>Peer Discovery Using etcd</doc:heading>

      <p>
        An <a href="https://coreos.com/etcd">etcd</a>-based discovery mechanism
        is available via <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-etcd">a plugin</a>. etcd v3 and v2 are supported.
      </p>

      <p>
        Nodes register with etcd on boot and unregister when they
        leave. Prior to registration, nodes will attempt to acquire a
        lock in etcd to reduce the probability of a <a href="#initial-formation-race-condition">race condition
        during initial cluster formation</a>.
      </p>

      <p>
        Nodes contact etcd periodically to refresh
        their keys. Those that haven't done so in a configurable
        period of time (node TTL) are cleaned up from etcd.  If
        configured, such nodes can be forcefully removed from the
        cluster.
      </p>

      <p>
        To use etcd for peer discovery, set the <code>cluster_formation.peer_discovery_backend</code>
        to <code>rabbit_peer_discovery_etcd</code> and provide an etcd node hostname for the plugin
        to connect to:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

# etcd host (hostname or IP address). This property is required or peer discovery won't be performed.
cluster_formation.etcd.host = etcd.eng.example.local
</pre>
      </p>

      <p>
        It is possible to configure etcd port and URI scheme:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

cluster_formation.etcd.host = etcd.eng.example.local
# 2379 is used by default
cluster_formation.etcd.port = 2379
# http is used by default
cluster_formation.etcd.scheme = http
</pre>
      </p>

      <p>
        etcd keys used by peer discovery will be prefixed with "rabbitmq" by default:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

cluster_formation.etcd.host = etcd.eng.example.local
# rabbitmq is used by default
cluster_formation.etcd.key_prefix = rabbitmq_discovery
</pre>
      </p>

      <p>
        Key used for node registration will have a TTL interval set for them. Online nodes
        will periodically refresh their key(s). The TTL value can be configured:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

cluster_formation.etcd.host = etcd.eng.example.local
# node TTL in seconds
# default: 30
cluster_formation.etcd.ttl = 40
</pre>

        Key refreshes will be performed every <code>TTL/2</code> seconds.
        It is possible to forcefully remove the nodes that fail to refresh their keys from the cluster.
        This is covered later in this guide.
      </p>

      <p>
        When a node tries to acquire a lock on boot and the lock is already taken,
        it will wait for the lock to be come available with a timeout. Default value is 300
        seconds but it can be configured:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

cluster_formation.etcd.host = etcd.eng.example.local
# lock acquisition timeout in seconds
# default: 300
cluster_formation.etcd.lock_wait_time = 60
</pre>
      </p>

      <p>
        If multiple RabbitMQ clusters share an etcd instance, each must use
        a unique cluster name:

        <pre class="sourcecode ini">
cluster_formation.peer_discovery_backend = rabbit_peer_discovery_etcd

cluster_formation.etcd.host = etcd.eng.example.local
# default name: "default"
cluster_formation.etcd.cluster_name = staging
</pre>
      </p>
    </doc:section>


    <doc:section name="initial-formation-race-condition">
      <doc:heading>Race Conditions During Initial Cluster Formation</doc:heading>

      <p>
        Consider a deployment
        where the entire cluster is provisioned at once and all nodes
        start in parallel. In this case there's a natural race
        condition between node registration and more than one node
        can become "first to register" (discovers no existing peers
        and thus starts as standalone).
      </p>
      <p>
        Different peer discovery backends use different approaches to
        minimize the probability of such scenario. Some use locking
        (etcd, Consul), others use a technique known as randomized startup delay.
        With randomized startup delay nodes will delay their startup
        for a randomly picked value (between 5 and 60 seconds by default).
      </p>
      <p>
        Some backends (config file, DNS) rely on a pre-configured set of peers and avoid
        the issue that way.
      </p>
      <p>
        Effective delay interval, if used, is logged on node boot.
      </p>
      <p>
        Lastly, some mechanism rely on ordered node startup provided by the underlying
        provisioning and orchestration tool. <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#understanding-stateful-pod-initialization">Kubernetes stateful sets</a> is one example
        of an environment that offers such a guarantee.
      </p>
    </doc:section>

    <doc:section name="node-health-checks-and-cleanup">
      <doc:heading>Node Health Checks and Cleanup</doc:heading>

      <p>
        Sometimes a node is a cluster member but not known to the discovery backend.
        For example, consider a cluster that uses the AWS backend configured to use autoscaling group membership.
        If an EC2 instance in that group fails and is later re-created, it will be considered
        an unavailable node in the RabbitMQ cluster. Such unknown nodes can be logged or forcefully
        removed from the cluster.
      </p>
      <p>
        Forced node removal can be dangerous and should be carefully considered. For example,
        a node that's temporarily unavailable but will be rejoining (or recreated with its
        persistent storage re-attached from its previous incarnation) can be kicked
        out of the cluster permanently by automatic cleanup, thus failing to rejoin.
      </p>
      <p>
        To log warnings for the unknown nodes,
        <code>cluster_formation.node_cleanup.only_log_warning</code> should be set to
        <code>true</code>. This is the default behavior.
      </p>
      <p>
        To forcefully delete the unknown nodes from the cluster,
        <code>cluster_formation.node_cleanup.only_log_warning</code> should be set to
        <code>false</code>. Note that this option should be used with care, in particular
        with discovery backends other than AWS.
      </p>

      <p>
        Some backends (Consul, etcd) support node health checks (or TTL).
        Nodes periodically notify their respective discovery service (e.g. Consul)
        that they are still available. If no notifications from a node come
        in after a period of time, the node is considered to be in the warning state.
        With etcd, such nodes will no longer show up in discovery results. With Consul,
        they can either be removed (deregistered) or their warning state can be
        reported. Please see documentation for those backends to learn more.
      </p>

      <p>
        Automatic cleanup of absent nodes makes most sense in environments where failed/discontinued nodes
        will be replaced with brand new ones (including cases when persistent storage won't be re-attached).
      </p>

      <p>
        When automatic node cleanup is disabled (switched to the warning mode), operators have to
        explicitly remove absent cluster nodes using <a href="/cli.html">CLI tools</a>.
      </p>
    </doc:section>


    <doc:section name="troubleshooting-cluster-formation">
      <doc:heading>Troubleshooting</doc:heading>

      <p>
        The peer discovery subsystem and individual mechanism implementations log important
        discovery procedure steps at the <code>info</code> log level. More extensive logging
        is available at the <code>debug</code> level. Mechanisms that depend on external services
        accessible over HTTP will log all outgoing HTTP requests and response codes at <code>debug</code> level.
        See the <a href="/logging.html">logging guide</a> for more information about logging configuration.
      </p>

      <p>
        If the log does not contain any entries that demonstrate peer discovery progress, for example, the list
        of nodes retrieved by the mechanism or clustering attempts, it may mean that the node already has
        an initialised data directory or is already a member of the cluster. In those cases peer discovery
        won't be performed.
      </p>

      <p>
        Peer discovery relies on inter-node network connectivity and successful authentication via a shared
        secret. Verifying that nodes can communicate with one another and use the expected Erlang cookie value (that's also identical across all cluster nodes). See the main <a href="clustering.html">Clustering guide</a> for more information.
      </p>

      <p>
        A methodology for network connectivty troubleshooting as well as commonly used tools are covered in the <a href="troubleshooting-networking.html">Troubleshooting Network Connectivity</a> guide.
      </p>
    </doc:section>
  </body>
</html>
