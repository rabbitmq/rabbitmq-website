<?xml-stylesheet type="text/xml" href="page.xsl"?>
<!DOCTYPE html PUBLIC "bug in xslt processor requires fake doctype"
"otherwise css isn't included" [
<!ENTITY % entities SYSTEM "rabbit.ent" >
%entities;
]>
<html xmlns:doc="http://www.rabbitmq.com/namespaces/ad-hoc/doc">
  <head>
    <title>RabbitMQ - High availability with Pacemaker and DRBD</title>
  </head>
  <body>
    <doc:div>
      <doc:toc class="compact">
	<doc:heading>Table of Contents</doc:heading>
      </doc:toc>

      <doc:section name="introduction">
	<doc:heading>Introduction</doc:heading>

        <p>There are many forms of high availability, replication and
        resilience in the face of various different types of
        failure. RabbitMQ can currently be made to work in an
        active/passive setup, such that persistent messages that have
        been written to disk on the active node are able to be
        recovered by the passive node should the active node
        fail. Non-persistent messages will be lost, and the promotion
        of the passive node may take a little while as it reads the
        messages off disk.
        </p>

        <p>Whilst RabbitMQ also supports clustering, clustering is
        intended to facilitate scalability, not availability. Thus in
        a cluster, if a node fails, queues which were on the failed
        node are lost. With the high availability setup described in
        this guide, when a node fails, the durable queues and the
        persistent messages within them can be recovered by a
        different node.</p>

        <p>Clustering can be combined with high availability to create
        a cluster that can scale beyond a single node and
        simultaneously preserve persistent messages and durable
        resources in the event of node failure.</p>
      </doc:section>

      <doc:section name="requirements">
	<doc:heading>Requirements</doc:heading>

        <p>This guide assumes that you're going to use the <a
        href="http://www.clusterlabs.org/">Pacemaker</a> HA stack to
        do the resource management and monitoring. This guide will
        also make use of <a href="http://www.drbd.org/">DRBD</a> to
        provide a shared storage area in which the active node will
        write messages. If you have a NAS or SAN or some other means
        of providing reliable shared storage to both nodes, then you
        can use that instead of DRBD.</p>

        <p>This guide does not tell you how to install Pacemaker,
        Heartbeat, OpenAIS, CoroSync or DRBD - there are already
        several good guides available for these tasks:</p>

        <p><ul class="plain">
          <li><a
          href="http://www.clusterlabs.org/wiki/Install">Pacemaker
          install guide</a></li>
	  <li>General <a
	  href="http://www.clusterlabs.org/wiki/Documentation">Pacemaker
	  documentation</a>: In particular, see the <i>Clusters from
	  scratch</i> guides</li>
          <li><a href="http://www.drbd.org/users-guide/">DRBD
          users guide</a></li>
        </ul></p>

        <p>Note that I used CoroSync (which is a cut-down version of
        OpenAIS) in preference to Heartbeat. However, I also had
        Heartbeat installed so as to be able to access Heartbeat's OCF
        scripts. You should find that the instructions here work
        equally well regardless of whether you use CoroSync, Heartbeat
        or OpenAIS as the Pacemaker underlying messaging layer.
        </p>

        <p>If you're compiling Pacemaker et al from source, be aware
        that the various autoconf configure scripts don't seem to test
        thoroughly enough for various libraries, the result of which
        is that compilation may eventually fail due to missing
        libraries, even though the configure script passes. Under
        Debian Sid, I found I had to install the following extra
        packages, though obviously your mileage will vary with
        different distributions: <code>autoconf libtool pkg-config
        libglib2.0-dev libxml2 libxml2-dev uuid-dev uuid-runtime
        libnss3-dev groff libxslt1.1 libxslt1-dev libesmtp5
        libesmtp-dev libssl-dev ssmping xsltproc</code>. Be aware that
        I found that if the build did fail, merely installing the
        necessary libraries was not enough to make the build then pass
        - I had to go back and re-run the autotools and configure
        steps before it would find the new libraries and compile
        correctly.</p>

      </doc:section>

      <doc:section name="assumption">
	<doc:heading>Assumptions</doc:heading>

        <p>By this point, I assume that you have installed Pacemaker
        and, if you're going to use it, DRBD, on two different
        machines, which can see each other. You should have configured
        DRBD and thus be able to set one node the primary and the
        other the secondary for the DRBD resource. The initial sync
        between the two nodes has already been done. My DRBD resource
        <code>drbd1</code> configuration looks like:</p>

<pre class="sourcecode">
resource drbd1 {
  device    /dev/drbd1;
  disk      /dev/vdb;
  meta-disk internal;
  on ha-node-1 {
    address   192.168.100.21:7789;
  }
  on ha-node-2 {
    address   192.168.100.22:7789;
  }
}
</pre>

        <p><code>crm configure show</code> should show nothing configured:</p>
<pre class="sourcecode">
ha-node-2:~# crm configure show
node ha-node-1
node ha-node-2
property $id="cib-bootstrap-options" \
        dc-version="1.0.7-6fab059e5cbe82faa65f5aac2042ecc0f2f535c7" \
        cluster-infrastructure="openais" \
        expected-quorum-votes="2" \
        stonith-enabled="false" \
        no-quorum-policy="ignore" \
        last-lrm-refresh="1268667122"
rsc_defaults $id="rsc-options" \
        resource-stickiness="100"
</pre>
        <p>Note that here I have already set stickiness to 100,
        disabled STONITH, and told Pacemaker not to worry about not
        having quorum (see the section <i>Create an Active/Passive
        cluster</i> in the <i>Clusters from scratch</i> guides
        referenced above). In reality, you probably do want STONITH
        devices, and you'll want a cluster of at least 3 nodes, so
        that quorum can be maintained in the event of node failure,
        and ring-fencing can occur.</p>
      </doc:section>

      <doc:section name="drbd_pacemaker">
	<doc:heading>DRBD in Pacemaker</doc:heading>

        <p>If you are planning on using DRBD to provide shared
        storage, you need to get Pacemaker to manage this. The
        instructions here are adapted for my setup but are pretty much
        the same as from the <i>Clusters from scratch</i> guides and
        from the <a
        href="http://www.drbd.org/users-guide/ch-pacemaker.html">DRBD
        Pacemaker documentation</a>. DRBD only allows one node at a
        time access to the shared device, so there is never any danger
        of multiple nodes writing over the same data. If you're using
        some other means to provide shared storage, you may wish to
        use Pacemaker to ensure only one node has the shared storage
        area mounted at any one time.</p>

<pre class="sourcecode">
ha-node-2:~# crm
crm(live)# cib new drbd
INFO: drbd shadow CIB created
crm(drbd)# configure primitive drbd ocf:linbit:drbd params drbd_resource="drbd1" op monitor interval="60s"
crm(drbd)# configure ms drbd_ms drbd meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
crm(drbd)# configure primitive drbd_fs ocf:heartbeat:Filesystem params device="/dev/drbd1" directory="/media/drbd1" fstype="ext3"
crm(drbd)# configure colocation fs_on_drbd inf: drbd_fs drbd_ms:Master
crm(drbd)# configure order fs_after_drbd inf: drbd_ms:promote drbd_fs:start
crm(drbd)# cib commit drbd
crm(drbd)# cib use live
crm(live)# cib delete drbd
crm(live)# exit
bye
ha-node-2:~#
</pre>

        <p>The first <code>configure</code> command here creates a
        resource <i>drbd</i>. That is then incorporated into a
        master+slave resource called <i>drbd_ms</i> in the second
        <code>configure</code>. We then create a <i>drbd_fs</i>
        resource which knows how to mount the DRBD device. We can only
        mount the DRBD resource on the node which is the master, so we
        then create a colocation directive <i>fs_on_drbd</i> which
        states that the <i>drbd_fs</i> and <i>drbd_ms:Master</i>
        resources must be on the same node. Finally, we know that we
        can only mount the file-system once the node has successfully
        become the master node for the DRBD device. Similarly, we must
        un-mount the DRBD device before degrading a node to the
        secondary for the DRBD device. Both of these are achieved by
        creating the order directive which says to promote the
        <i>drbd_ms</i> resource before starting the <i>drbd_fs</i>
        resource.</p>

        <p>If you're not using DRBD, you will just want to configure a
        single primitive <code>ocf:heartbeat:Filesystem</code> which
        can mount the shared storage.</p>
      </doc:section>

      <doc:section name="simple_rabbit">
	<doc:heading>Simple HA Rabbit</doc:heading>

        <p>The main trick to HA Rabbit is to ensure that when the
        passive node becomes the active node, it must have the same
        node-name as the failed node. It must also have read and write
        access to the files in the shared storage, and if it's going
        to also be part of a cluster then it must also have the same
        Erlang cookie.</p>

        <p>Start by installing the RabbitMQ server on both nodes. The
        server runs as the user <code>rabbitmq</code> which is a
        member of the group <code>rabbitmq</code>. You must ensure
        that this user and group have the same UIDs and GIDs on both
        nodes. If necessary, edit <code>/etc/passwd</code> and
        <code>/etc/group</code>. (You can probably save yourself some
        time by explicitly creating the <code>rabbitmq</code> user and
        group with the same UID and GID on all nodes before installing
        the RabbitMQ server at all). Once done, reinstall the RabbitMQ
        server which will ensure all files are owned by the correct
        user and group (assuming you're installing from a binary
        package. If you're installing from source, you probably know
        what to do anyway). Also make sure the <code>rabbitmq</code>
        user has permission to write and read to the shared storage
        mount point (<code>/media/drbd1</code> in my example
        above). Although not strictly necessary at this stage, next
        ensure that all the nodes share the same Erlang cookie. The
        <code>rabbitmq</code> home directory is normally
        <code>/var/lib/rabbitmq</code>, so:</p>

<pre class="sourcecode">
ha-node-2:~# scp /var/lib/rabbitmq/.erlang.cookie ha-node-1:/var/lib/rabbitmq/

ha-node-1:~# chown rabbitmq: /var/lib/rabbitmq/.erlang.cookie
ha-node-1:~# chmod 400 /var/lib/rabbitmq/.erlang.cookie
</pre>

        <p>We also need to make sure that when the nodes boot, they
        don't start Rabbit. Thus edit the init script (usually at
        <code>/etc/init.d/rabbitmq-server</code>) and just insert an
        <code>exit 0</code> after the comments at the top (the more
        <i>correct</i> solution is to use something like
        <code>update-rc.d rabbitmq-server disable S 2 3 4 5</code>,
        depending on your platform). Now create a resource in
        Pacemaker for Rabbit:</p>

<pre class="sourcecode">
ha-node-2:~# crm
crm(live)# cib new bunny
INFO: bunny shadow CIB created
crm(bunny)# configure primitive bunny ocf:rabbitmq:rabbitmq-server params mnesia_base="/media/drbd1"
crm(bunny)# configure colocation bunny_on_fs inf: bunny drbd_fs
crm(bunny)# configure order bunny_after_fs inf: drbd_fs bunny
crm(bunny)# cib commit bunny
crm(bunny)# cib use live
crm(live)# cib delete bunny
crm(live)# exit
bye
ha-node-2:~#
</pre>

        <p>We create a resource called <i>bunny</i> which is our
        RabbitMQ instance. We configure it to place its database
        files (and message store) on our DRBD-backed mount point. We
        then need to colocate the <i>bunny</i> resource with the
        <i>drbd_fs</i> resource, and we also need to make sure that
        the file-system is mounted before we start the <i>bunny</i>
        resource (and similarly, the <i>bunny</i> resource is stopped
        before we un-mount the file-system).</p>

        <p>By default, the <code>ocf:rabbitmq:rabbitmq-server</code>
        script sets the node-name to <code>rabbit@localhost</code> on
        all nodes. This is the simplest solution as it should always
        be possible to resolve <code>localhost</code>, but it does
        mean that you can't join a cluster. We'll correct that later
        on.</p>

        <p>You should now find that one node has the DRBD master, the
        file-system mounted, and rabbit running. If you stop that node
        (turn it off, or just stop Heartbeat or CoroSync), you should
        find all the services migrate to the other node. If you create
        some durable resources (e.g. durable queues or exchanges), you
        should find that these survive the transition to the other
        node. It's a good idea to play with stopping and starting the
        nodes at this point to build confidence that it really is
        doing the right thing.</p>
      </doc:section>

      <doc:section name="with_ip">
	<doc:heading>Getting an IP address to move with Rabbit</doc:heading>

        <p>You may wish your highly-available Rabbit to always be
        accessible on the same IP address. There are a number of ways
        to achieve this, for example, you could use a TCP load
        balancer. Here, I demonstrate getting Pacemaker to migrate an
        IP address with Rabbit. Note that if you're happy to build
        into your clients the various IPs on which Rabbit may be
        available, then you may not need this step at all.</p>

        <p>In testing, I have found that the IP migration done by
        Pacemaker frequently hangs if the IP address being migrated is
        on the same network as another IP interface on the same
        node. In the example here, the two nodes have IP
        addresses of 192.168.100.21 and 192.168.100.22 (both on a
        /24). I shall therefore choose that the IP on which Rabbit
        will always be available is 192.168.50.1 (i.e. a different
        network). This seems to make the migration of the IP address
        more reliable.</p>

<pre class="sourcecode">
ha-node-2:~# crm
crm(live)# cib new ip
INFO: bunny shadow CIB created
crm(ip)# configure primitive ip ocf:heartbeat:IPaddr2 params ip="192.168.50.1" cidr_netmask="24"
crm(ip)# configure colocation bunny_on_ip inf: bunny ip
crm(ip)# configure order bunny_after_ip inf: ip bunny
crm(ip)# cib commit ip
crm(ip)# cib use live
crm(live)# cib delete ip
crm(live)# exit
bye
ha-node-2:~#
</pre>

        <p>This should all be looking rather familiar by now. We
        create the <i>ip</i> resource, we say that the <i>bunny</i>
        resource must run on the same node as the <i>ip</i> resource,
        and we make sure that the <i>bunny</i> resource is started
        after, and stopped before, the <i>ip</i> resource. By default,
        the RabbitMQ server will listen on all network interfaces, so
        we are done now, but we can explicitly tell it to listen on
        our chosen IP:</p>

<pre class="sourcecode">
ha-node-2:~# crm_resource --resource bunny --set-parameter ip --parameter-value 192.168.50.1
ha-node-2:~#
</pre>

        <p>The above command is the same as going into
        <code>crm</code>, calling <code>configure edit bunny</code>
        and then in the editor that will appear, adding to the
        <i>params</i> <code>ip="192.168.50.1"</code>. The RabbitMQ
        server should then immediately be restarted, as Pacemaker
        detects that its configuration has changed. You should now
        find that when a node fails, all the resources get transferred
        over to the spare node, and Rabbit will start up on the same
        IP address. Thus clients can be completely oblivious as to
        which node is the active node, and which is the passive
        node. You could also indicate to Rabbit the IP address to use
        by editing the <code>/etc/rabbitmq/rabbitmq.config</code> file
        and providing an entry for
        <code>tcp_listeners</code>. However, that would then spread
        the configuration across multiple systems, and would require
        you to both synchronise the configuration across all the HA
        nodes, and to restart the <i>bunny</i> resource.</p>
      </doc:section>

      <doc:section name="ha_cluster">
	<doc:heading>HA within a RabbitMQ Cluster</doc:heading>

        <p>Finally, we wish to be able to make our HA Rabbit join a
        cluster of Rabbits. For simplicity, this example will join a
        single other RabbitMQ node into our HA Rabbit. It is possible
        to extend this to have, say, a cluster of 4 RabbitMQ servers
        in a cluster spread across, say, 6 nodes, all managed by
        Pacemaker. The key to doing this is to create multiple units
        of shared-storage, file-system, IP address and RabbitMQ server
        which are clustered together. At this point DRBD becomes the
        limiting factor as DRBD can only work with one master and one
        slave, whilst here we need multiple slaves, thus for the more
        complex scenario, you will likely need the use of a SAN or NAS
        or similar to provide shared storage.</p>

        <p>Rabbit nodes within a cluster need to be able to resolve
        each others' host-name. Currently we have that the node-name is
        set to <code>rabbit@localhost</code> which must be changed as
        the other nodes in the cluster would get the wrong idea if we
        asked them to resolve <i>localhost</i>. We can't use the IP
        address raw (i.e. we can't have the node-name as
        <code>rabbit@192.168.50.1</code>) without switching to
        long-names, which is more work, so instead our plan is to give
        a host-name of rabbit-ha-1 to 192.168.50.1 and then set the
        host-name to <code>rabbit@rabbit-ha-1</code>. There are a
        number of ways to do this - either configure this in your DNS
        server, or edit enough <code>/etc/hosts</code> files to ensure
        that both of the HA nodes, and the other node which we will
        join into the cluster can all resolve <i>rabbit-ha-1</i>.</p>

        <p>Having done that, you should now be able to issue:</p>

<pre class="sourcecode">
ha-node-2:~# crm_resource --resource bunny --set-parameter nodename --parameter-value "rabbit@rabbit-ha-1"
ha-node-2:~#
</pre>

        <p>Again, the <i>bunny</i> resource should instantly restart,
        reflecting the change in node-name. Now our non-HA node is
        given the exciting name of <code>non-ha</code>. Rabbit has
        been installed on this node and is currently running with the
        node-name <code>rabbit@non-ha</code>. Both the HA nodes can
        resolve the host-name <code>non-ha</code>. As per the <a
        href="clustering.html">clustering guide</a>, we have already
        ensured that it has the same Erlang cookie as the HA nodes do,
        and we issue the following commands to join it into the
        cluster:</p>

<pre class="sourcecode">
non-ha:~# rabbitmqctl stop_app
non-ha:~# rabbitmqctl reset
non-ha:~# rabbitmqctl cluster rabbit@non-ha rabbit@rabbit-ha-1
non-ha:~# rabbitmqctl start_app
non-ha:~# rabbitmqctl status
Status of node 'rabbit@non-ha' ...
[{running_applications,[{rabbit,"RabbitMQ","%%VSN%%"},
                        {mnesia,"MNESIA  CXC 138 12","4.4.12"},
                        {os_mon,"CPO  CXC 138 46","2.2.4"},
                        {sasl,"SASL  CXC 138 11","2.1.8"},
                        {stdlib,"ERTS  CXC 138 10","1.16.4"},
                        {kernel,"ERTS  CXC 138 10","2.13.4"}]},
 {nodes,['rabbit@rabbit-ha-1','rabbit@non-ha']},
 {running_nodes,['rabbit@rabbit-ha-1','rabbit@non-ha']}]
...done.
non-ha:~#
</pre>

        <p>And that's it - you should find that if you kill the HA
        node which is running Rabbit then the resources should
        transfer over to the passive node, Rabbit will start up, and
        will successfully reconnect back into the cluster.</p>
      </doc:section>

      <doc:section name="params">
	<doc:heading>Configuration Parameters</doc:heading>

        <p>The configuration parameters that can be provided in
        Pacemaker to the <code>ocf:rabbitmq:rabbitmq-server</code>
        script are all logical translations of the
        <code>RABBITMQ_*</code> environment variables that can be used
        to configure Rabbit. They are:</p>
        <dl>
          <dt>multi</dt>
          <dd>The path to the rabbitmq-multi script </dd>

          <dt>ctl</dt>
          <dd>The path to the rabbitmqctl script</dd>

          <dt>nodename</dt>
          <dd>The node name for rabbitmq-server</dd>

          <dt>ip</dt>
          <dd>The IP address for rabbitmq-server to listen on</dd>

          <dt>port</dt>
          <dd>The IP Port for rabbitmq-server to listen on</dd>

          <dt>cluster_config_file</dt>
          <dd>Location of the cluster config file</dd>

          <dt>config_file</dt>
          <dd>Location of the config file</dd>

          <dt>log_base</dt>
          <dd>Location of the directory under which logs will be created</dd>

          <dt>mnesia_base</dt>
          <dd>Location of the directory under which mnesia will store data</dd>

          <dt>server_start_args</dt>
          <dd>Additional arguments provided to the server on startup</dd>
        </dl>
      </doc:section>

    </doc:div>
  </body>
</html>
